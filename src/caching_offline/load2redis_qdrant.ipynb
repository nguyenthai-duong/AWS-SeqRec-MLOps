{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duong/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 487.56it/s]  \n",
      "2025/07/27 09:51:14 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.4.1+cu121'\n",
      "/home/duong/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/torch/serialization.py:1074: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 150.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model v1 (run_id=aac89aa42a3c447eac1294546df933e6) and IDMapper.\n",
      "\n",
      "üîπ Embedding for 'B00000IV95' (index 36): [ 0.05149313  0.0443965  -0.01442701  0.07874709  0.07239346]\n",
      "\n",
      "üîπ Top 5 similar items to 'B00000IV95':\n",
      " - B0C3H818H4 (index 4009): score = 2.2501\n",
      " - B0C48KPLZ2 (index 4026): score = 1.6529\n",
      " - 0975277324 (index 5): score = 1.6294\n",
      " - B0C1FX3BGK (index 3976): score = 1.5719\n",
      " - B00000IV35 (index 35): score = 1.5157\n",
      "\n",
      "üîπ Batch prediction scores:\n",
      " - (B00000IV95, B0792X1RSC): score = 0.5309\n",
      " - (B0792X1RSC, B00000IV95): score = 0.5309\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from typing import List, Tuple, Dict\n",
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "\n",
    "\n",
    "def configure_mlflow(\n",
    "    s3_endpoint: str, aws_key: str, aws_secret: str, tracking_uri: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Configures MLflow to use a specified tracking URI and S3-compatible storage (e.g., MinIO).\n",
    "\n",
    "    Args:\n",
    "        s3_endpoint (str): Endpoint URL for the S3-compatible storage (e.g., MinIO).\n",
    "        aws_key (str): AWS access key ID for S3 authentication.\n",
    "        aws_secret (str): AWS secret access key for S3 authentication.\n",
    "        tracking_uri (str): URI for the MLflow tracking server.\n",
    "    \"\"\"\n",
    "    os.environ.update(\n",
    "        {\n",
    "            \"AWS_ACCESS_KEY_ID\": aws_key,\n",
    "            \"AWS_SECRET_ACCESS_KEY\": aws_secret,\n",
    "            \"MLFLOW_S3_ENDPOINT_URL\": s3_endpoint,\n",
    "            \"MLFLOW_S3_IGNORE_TLS\": \"true\",\n",
    "        }\n",
    "    )\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "    session = boto3.session.Session(\n",
    "        aws_access_key_id=aws_key, aws_secret_access_key=aws_secret\n",
    "    )\n",
    "    S3ArtifactRepository._get_s3_client = lambda self: session.client(\n",
    "        \"s3\", endpoint_url=s3_endpoint\n",
    "    )\n",
    "\n",
    "\n",
    "class SimpleIDMapper:\n",
    "    \"\"\"\n",
    "    A class for mapping item IDs to indices and vice versa using a JSON mapping file.\n",
    "\n",
    "    Attributes:\n",
    "        item_to_index (Dict[str, int]): Dictionary mapping item IDs to their indices.\n",
    "        index_to_item (Dict[int, str]): Dictionary mapping indices to item IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mapping_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SimpleIDMapper with a JSON mapping file.\n",
    "\n",
    "        Args:\n",
    "            mapping_path (str): Path to the JSON file containing item-to-index mappings.\n",
    "        \"\"\"\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index: Dict[str, int] = mapping[\"item_to_index\"]\n",
    "        self.index_to_item: Dict[int, str] = mapping[\"index_to_item\"]\n",
    "\n",
    "\n",
    "def load_champion_model(\n",
    "    model_name: str,\n",
    ") -> Tuple[torch.jit.ScriptModule, SimpleIDMapper]:\n",
    "    \"\"\"\n",
    "    Loads the latest champion model and its associated ID mapper from MLflow.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the registered model in MLflow.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.jit.ScriptModule, SimpleIDMapper]: Loaded TorchScript model and ID mapper.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If no champion model version is found.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    champs = [v for v in versions if v.tags.get(\"champion\", \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise RuntimeError(f\"No champion version found for model '{model_name}'\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model = mlflow.pytorch.load_model(f\"models:/{model_name}/{champ.version}\")\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    print(f\"Loaded model v{champ.version} (run_id={run_id}) and IDMapper.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "\n",
    "def get_item_embedding(\n",
    "    model: torch.jit.ScriptModule,\n",
    "    id_mapper: SimpleIDMapper,\n",
    "    item_id: str,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Retrieves the embedding for a given item ID using the provided model and ID mapper.\n",
    "\n",
    "    Args:\n",
    "        model (torch.jit.ScriptModule): Loaded TorchScript model.\n",
    "        id_mapper (SimpleIDMapper): ID mapper for converting item IDs to indices.\n",
    "        item_id (str): Item ID to retrieve the embedding for.\n",
    "        device (torch.device): Device to perform computation on. Defaults to CPU.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Embedding vector for the specified item.\n",
    "    \"\"\"\n",
    "    idx = torch.tensor([id_mapper.item_to_index[item_id]], device=device)\n",
    "    return model.embeddings(idx).squeeze(0)\n",
    "\n",
    "\n",
    "def get_topk_similar(\n",
    "    model: torch.jit.ScriptModule,\n",
    "    id_mapper: SimpleIDMapper,\n",
    "    item_id: str,\n",
    "    top_k: int = 10,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> List[Tuple[str, int, float]]:\n",
    "    \"\"\"\n",
    "    Finds the top-K similar items to a given item ID based on embedding similarity.\n",
    "\n",
    "    Args:\n",
    "        model (torch.jit.ScriptModule): Loaded TorchScript model.\n",
    "        id_mapper (SimpleIDMapper): ID mapper for converting item IDs to indices.\n",
    "        item_id (str): Item ID to find similar items for.\n",
    "        top_k (int): Number of similar items to return. Defaults to 10.\n",
    "        device (torch.device): Device to perform computation on. Defaults to CPU.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int, float]]: List of tuples containing similar item IDs, their indices, and similarity scores.\n",
    "    \"\"\"\n",
    "    target_idx = id_mapper.item_to_index[item_id]\n",
    "    weight = model.embeddings.weight[:-1].to(device)  # Exclude padding_idx\n",
    "    with torch.no_grad():\n",
    "        target_emb = weight[target_idx]\n",
    "        sims = torch.matmul(weight, target_emb)\n",
    "    sims[target_idx] = -np.inf  # Exclude self\n",
    "    topk = torch.topk(sims, k=top_k)\n",
    "    return [\n",
    "        (id_mapper.index_to_item[int(i)], int(i), float(s))\n",
    "        for i, s in zip(topk.indices, topk.values)\n",
    "    ]\n",
    "\n",
    "\n",
    "def predict_batch(\n",
    "    model: torch.jit.ScriptModule,\n",
    "    id_mapper: SimpleIDMapper,\n",
    "    batch: Dict[str, List[str]],\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Performs batch prediction for target and context item pairs using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.jit.ScriptModule): Loaded TorchScript model.\n",
    "        id_mapper (SimpleIDMapper): ID mapper for converting item IDs to indices.\n",
    "        batch (Dict[str, List[str]]): Dictionary containing lists of target and context item IDs.\n",
    "        device (torch.device): Device to perform computation on. Defaults to CPU.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of prediction scores for the batch.\n",
    "    \"\"\"\n",
    "    tgt = torch.tensor(\n",
    "        [id_mapper.item_to_index[i] for i in batch[\"target_items\"]], device=device\n",
    "    )\n",
    "    ctx = torch.tensor(\n",
    "        [id_mapper.item_to_index[i] for i in batch[\"context_items\"]], device=device\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        return model(tgt, ctx).cpu().numpy()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate model loading, embedding retrieval, and prediction.\n",
    "\n",
    "    Configures MLflow, loads the champion model and ID mapper, and performs example\n",
    "    embedding retrieval, similarity search, and batch prediction.\n",
    "    \"\"\"\n",
    "    configure_mlflow(\n",
    "        s3_endpoint=\"http://127.0.0.1:9010\",\n",
    "        aws_key=\"admin\",\n",
    "        aws_secret=\"Password1234\",\n",
    "        tracking_uri=\"http://localhost:5002\",\n",
    "    )\n",
    "\n",
    "    model_name = \"item2vec_skipgram\"\n",
    "    model, id_mapper = load_champion_model(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    test_item = \"B00000IV95\"\n",
    "    test_idx = id_mapper.item_to_index[test_item]\n",
    "    emb = get_item_embedding(model, id_mapper, test_item, device)\n",
    "    print(\n",
    "        f\"Embedding for '{test_item}' (index {test_idx}): {emb[:5].detach().cpu().numpy()}\"\n",
    "    )\n",
    "\n",
    "    topk = get_topk_similar(model, id_mapper, test_item, top_k=5, device=device)\n",
    "    print(f\"Top 5 similar items to '{test_item}':\")\n",
    "    for iid, idx, score in topk:\n",
    "        print(f\" - {iid} (index {idx}): score = {score:.4f}\")\n",
    "\n",
    "    batch = {\n",
    "        \"target_items\": [test_item, \"B0792X1RSC\"],\n",
    "        \"context_items\": [\"B0792X1RSC\", test_item],\n",
    "    }\n",
    "    scores = predict_batch(model, id_mapper, batch, device)\n",
    "    print(f\"Batch prediction scores:\")\n",
    "    for t, c, s in zip(batch[\"target_items\"], batch[\"context_items\"], scores):\n",
    "        print(f\" - ({t}, {c}): score = {s:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941bd42",
   "metadata": {},
   "source": [
    "## Load embedding to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b76cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting embedding indexing...\n",
      "üì¶ Loading model from URI: models:/item2vec_skipgram/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 526.20it/s]  \n",
      "2025/07/27 09:51:15 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.4.1+cu121'\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 136.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model version 1 (run_id=aac89aa42a3c447eac1294546df933e6) with 4143 items.\n",
      "‚úÖ Extracted embeddings with shape (4143, 256)\n",
      "üåê Connecting to Qdrant at http://localhost:6333\n",
      "üóëÔ∏è Deleted existing collection 'item2vec_collection'.\n",
      "‚úÖ Created new collection 'item2vec_collection' with dim=256\n",
      "‚úÖ Upserted 4143 embeddings into Qdrant collection 'item2vec_collection'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "import boto3\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ROOT = \"/home/duong/Documents/datn1/src/model_item2vec\"\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MLflow configuration\n",
    "S3_ENDPOINT = \"http://127.0.0.1:9010\"\n",
    "AWS_KEY = \"admin\"\n",
    "AWS_SECRET = \"Password1234\"\n",
    "TRACKING_URI = \"http://localhost:5002\"\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_KEY\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = S3_ENDPOINT\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "# Configure boto3 for MinIO\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=AWS_KEY, aws_secret_access_key=AWS_SECRET\n",
    ")\n",
    "S3ArtifactRepository._get_s3_client = lambda self: session.client(\n",
    "    \"s3\", endpoint_url=S3_ENDPOINT\n",
    ")\n",
    "\n",
    "# Qdrant configuration\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "QDRANT_COLLECTION = os.getenv(\"QDRANT_COLLECTION_NAME\", \"item2vec_collection\")\n",
    "\n",
    "MODEL_NAME = \"item2vec_skipgram\"\n",
    "TAG_NAME = \"champion\"\n",
    "\n",
    "\n",
    "class SimpleIDMapper:\n",
    "    \"\"\"A class to handle mapping between item IDs and indices.\n",
    "\n",
    "    Attributes:\n",
    "        item_to_index (dict): Mapping from item IDs to indices.\n",
    "        index_to_item (dict): Mapping from indices to item IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mapping_path: str):\n",
    "        \"\"\"Initialize the SimpleIDMapper with a JSON mapping file.\n",
    "\n",
    "        Args:\n",
    "            mapping_path (str): Path to the JSON file containing item-to-index mappings.\n",
    "        \"\"\"\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index = mapping[\"item_to_index\"]\n",
    "        self.index_to_item = mapping[\"index_to_item\"]\n",
    "\n",
    "\n",
    "def load_model_from_mlflow(model_name: str, tag: str = \"champion\") -> tuple:\n",
    "    \"\"\"Load a model and its ID mapper from MLflow.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model registered in MLflow.\n",
    "        tag (str, optional): Tag to filter model versions. Defaults to \"champion\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the loaded PyTorch model and SimpleIDMapper instance.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no model version with the specified tag is found.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    champs = [v for v in versions if v.tags.get(tag, \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise ValueError(f\"No version tagged '{tag}' found for model '{model_name}'.\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model_uri = f\"models:/{model_name}/{champ.version}\"\n",
    "    print(f\"Loading model from URI: {model_uri}\")\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    print(\n",
    "        f\"Loaded model version {champ.version} (run_id={run_id}) with {len(id_mapper.item_to_index)} items.\"\n",
    "    )\n",
    "    return model, id_mapper\n",
    "\n",
    "\n",
    "def get_all_embeddings(model, id_mapper: SimpleIDMapper) -> tuple:\n",
    "    \"\"\"Extract embeddings for all items using the provided model and ID mapper.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model with an embeddings layer.\n",
    "        id_mapper (SimpleIDMapper): Mapper for item IDs to indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the list of item IDs and their corresponding embeddings as a NumPy array.\n",
    "    \"\"\"\n",
    "    item_ids = list(id_mapper.item_to_index.keys())\n",
    "    item_indices = [id_mapper.item_to_index[id_] for id_ in item_ids]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    tensor_indices = torch.tensor(item_indices, device=device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.embeddings(tensor_indices).detach().cpu().numpy()\n",
    "    print(f\"Extracted embeddings with shape {embeddings.shape}\")\n",
    "    return item_ids, embeddings\n",
    "\n",
    "\n",
    "def index_embeddings_to_qdrant(\n",
    "    item_ids: list, embeddings: np.ndarray, qdrant_url: str, collection_name: str\n",
    "):\n",
    "    \"\"\"Index item embeddings into a Qdrant collection.\n",
    "\n",
    "    Args:\n",
    "        item_ids (list): List of item IDs.\n",
    "        embeddings (np.ndarray): Array of item embeddings.\n",
    "        qdrant_url (str): URL of the Qdrant server.\n",
    "        collection_name (str): Name of the Qdrant collection.\n",
    "\n",
    "    Raises:\n",
    "        UnexpectedResponse: If Qdrant returns an unexpected response (e.g., 404 or 400).\n",
    "        Exception: For other errors during indexing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = QdrantClient(url=qdrant_url)\n",
    "        print(f\"Connecting to Qdrant at {qdrant_url}\")\n",
    "\n",
    "        vector_dim = embeddings.shape[1]\n",
    "\n",
    "        # Delete collection if it exists\n",
    "        if client.collection_exists(collection_name):\n",
    "            client.delete_collection(collection_name)\n",
    "            print(f\"Deleted existing collection '{collection_name}'.\")\n",
    "\n",
    "        # Create new collection\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_dim, distance=Distance.COSINE),\n",
    "        )\n",
    "        print(f\"Created new collection '{collection_name}' with dim={vector_dim}\")\n",
    "\n",
    "        # Prepare points using sequential IDs\n",
    "        points = [\n",
    "            PointStruct(id=idx, vector=vec.tolist(), payload={\"item_id\": item_id})\n",
    "            for idx, (item_id, vec) in enumerate(zip(item_ids, embeddings))\n",
    "        ]\n",
    "\n",
    "        # Upsert points\n",
    "        upsert_result = client.upsert(collection_name=collection_name, points=points)\n",
    "        assert str(upsert_result.status) == \"completed\"\n",
    "        print(\n",
    "            f\"Upserted {len(points)} embeddings into Qdrant collection '{collection_name}'\"\n",
    "        )\n",
    "\n",
    "    except UnexpectedResponse as e:\n",
    "        print(\"Qdrant returned unexpected response (possibly 404 or 400).\")\n",
    "        print(\"Check that the endpoint is correct and Qdrant is running.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(\"Failed to index embeddings to Qdrant.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load a model, extract embeddings, and index them to Qdrant.\"\"\"\n",
    "    print(\"Starting embedding indexing...\")\n",
    "    model, id_mapper = load_model_from_mlflow(MODEL_NAME, TAG_NAME)\n",
    "    item_ids, embeddings = get_all_embeddings(model, id_mapper)\n",
    "    index_embeddings_to_qdrant(item_ids, embeddings, QDRANT_URL, QDRANT_COLLECTION)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489b078",
   "metadata": {},
   "source": [
    "### Test Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5fd1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting similarity search...\n",
      "üì¶ Loading model from URI: models:/item2vec_skipgram/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 468.61it/s]  \n",
      "2025/07/27 09:51:16 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.4.1+cu121'\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 102.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model version 1 (run_id=aac89aa42a3c447eac1294546df933e6) with 4143 items.\n",
      "‚úÖ Generated embedding for 'B00EMGM1JQ' with shape (256,)\n",
      "üåê Connecting to Qdrant at http://localhost:6333\n",
      "\n",
      "üîç Top 5 items similar to 'B00EMGM1JQ':\n",
      "üü¢ B00EMGM1JQ      | Score: 1.0000\n",
      "üü¢ B00C6Q4HEQ      | Score: 0.7464\n",
      "üü¢ B00C6Q5S44      | Score: 0.6372\n",
      "üü¢ B00C6Q1Z6E      | Score: 0.6326\n",
      "üü¢ B00CQHZ2LW      | Score: 0.6079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import mlflow\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "import boto3\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "QDRANT_COLLECTION = \"item2vec_collection\"\n",
    "MODEL_NAME = \"item2vec_skipgram\"\n",
    "TAG_NAME = \"champion\"\n",
    "ITEM_ID = \"B00EMGM1JQ\"\n",
    "TOP_K = 5\n",
    "\n",
    "S3_ENDPOINT = \"http://127.0.0.1:9010\"\n",
    "AWS_KEY = \"admin\"\n",
    "AWS_SECRET = \"Password1234\"\n",
    "TRACKING_URI = \"http://localhost:5002\"\n",
    "\n",
    "# Configure MLflow and MinIO\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_KEY\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = S3_ENDPOINT\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=AWS_KEY, aws_secret_access_key=AWS_SECRET\n",
    ")\n",
    "S3ArtifactRepository._get_s3_client = lambda self: session.client(\n",
    "    \"s3\", endpoint_url=S3_ENDPOINT\n",
    ")\n",
    "\n",
    "\n",
    "# === SimpleIDMapper ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index = mapping[\"item_to_index\"]\n",
    "        self.index_to_item = mapping[\"index_to_item\"]\n",
    "\n",
    "\n",
    "# === STEP 1: Load model and ID mapping ===\n",
    "def load_model_and_mapping(model_name: str, tag: str = \"champion\") -> tuple:\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    champs = [v for v in versions if v.tags.get(tag, \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå No version tagged '{tag}' found for model '{model_name}'.\"\n",
    "        )\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model_uri = f\"models:/{model_name}/{champ.version}\"\n",
    "    print(f\"üì¶ Loading model from URI: {model_uri}\")\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    print(\n",
    "        f\"‚úÖ Loaded model version {champ.version} (run_id={run_id}) with {len(id_mapper.item_to_index)} items.\"\n",
    "    )\n",
    "    return model, id_mapper\n",
    "\n",
    "\n",
    "# === STEP 2: Get embedding vector for ITEM_ID ===\n",
    "def get_item_embedding(\n",
    "    model, id_mapper: SimpleIDMapper, item_id: str, device: torch.device\n",
    ") -> np.ndarray:\n",
    "    idx = id_mapper.item_to_index.get(item_id)\n",
    "    if idx is None:\n",
    "        raise ValueError(f\"‚ùå Item ID '{item_id}' not found in ID mapping\")\n",
    "    tensor_idx = torch.tensor([idx], device=device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.embeddings(tensor_idx).detach().cpu().numpy()[0]\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# === STEP 3: Query Qdrant with embedding vector ===\n",
    "def search_similar_items(\n",
    "    qdrant_url: str, collection_name: str, embedding: np.ndarray, top_k: int\n",
    ") -> list:\n",
    "    try:\n",
    "        qdrant = QdrantClient(url=qdrant_url)\n",
    "        print(f\"üåê Connecting to Qdrant at {qdrant_url}\")\n",
    "        if not qdrant.collection_exists(collection_name):\n",
    "            raise ValueError(\n",
    "                f\"‚ùå Collection '{collection_name}' does not exist in Qdrant\"\n",
    "            )\n",
    "\n",
    "        results = qdrant.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=embedding.tolist(),\n",
    "            limit=top_k,\n",
    "        )\n",
    "        return results\n",
    "    except UnexpectedResponse as e:\n",
    "        print(\"‚ùå Qdrant returned unexpected response (possibly 404 or 400).\")\n",
    "        print(\"‚û°Ô∏è Check that the endpoint is correct and Qdrant is running.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Failed to query Qdrant.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    print(\"üöÄ Starting similarity search...\")\n",
    "    # Load model and ID mapping\n",
    "    model, id_mapper = load_model_and_mapping(MODEL_NAME, TAG_NAME)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Get embedding for ITEM_ID\n",
    "    embedding = get_item_embedding(model, id_mapper, ITEM_ID, device)\n",
    "    print(f\"‚úÖ Generated embedding for '{ITEM_ID}' with shape {embedding.shape}\")\n",
    "\n",
    "    # Query Qdrant\n",
    "    results = search_similar_items(QDRANT_URL, QDRANT_COLLECTION, embedding, TOP_K)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nüîç Top {TOP_K} items similar to '{ITEM_ID}':\")\n",
    "    for r in results:\n",
    "        item = r.payload.get(\"item_id\", \"<missing>\")\n",
    "        print(f\"üü¢ {item:15} | Score: {r.score:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f64b16",
   "metadata": {},
   "source": [
    "## Load pre-recommend to Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e711ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 392.82it/s]  \n",
      "2025/07/27 09:51:16 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.4.1+cu121'\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model v1 (run_id=aac89aa42a3c447eac1294546df933e6) and IDMapper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:16<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed. Stored 4143 recommendations to Redis and ../../data/batch_recs.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import redis\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import sys\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm.auto import tqdm\n",
    "import boto3\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "PROJECT_ROOT = \"/home/duong/Documents/datn1/src/model_item2vec\"\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "CONFIG = {\n",
    "    \"mlf_model_name\": \"item2vec_skipgram\",\n",
    "    \"qdrant_url\": f\"{os.getenv('QDRANT_HOST', 'localhost')}:{os.getenv('QDRANT_PORT', '6333')}\",\n",
    "    \"qdrant_collection_name\": os.getenv(\n",
    "        \"QDRANT_COLLECTION_NAME\", \"item2vec_collection\"\n",
    "    ),\n",
    "    \"redis_host\": os.getenv(\"REDIS_HOST\", \"localhost\"),\n",
    "    \"redis_port\": 6379,\n",
    "    \"redis_db\": int(os.getenv(\"REDIS_DB\", 0)),\n",
    "    \"batch_size\": 256,\n",
    "    \"top_k\": 10,\n",
    "    \"top_K\": 100,\n",
    "    \"output_file\": \"../../data/batch_recs.jsonl\",\n",
    "    \"s3_endpoint\": \"http://127.0.0.1:9010\",\n",
    "    \"aws_key\": \"admin\",\n",
    "    \"aws_secret\": \"Password1234\",\n",
    "}\n",
    "\n",
    "\n",
    "# === ENV SETUP ===\n",
    "def configure_mlflow():\n",
    "    os.environ.update(\n",
    "        {\n",
    "            \"AWS_ACCESS_KEY_ID\": CONFIG[\"aws_key\"],\n",
    "            \"AWS_SECRET_ACCESS_KEY\": CONFIG[\"aws_secret\"],\n",
    "            \"MLFLOW_S3_ENDPOINT_URL\": CONFIG[\"s3_endpoint\"],\n",
    "            \"MLFLOW_S3_IGNORE_TLS\": \"true\",\n",
    "        }\n",
    "    )\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5002\")\n",
    "\n",
    "    session = boto3.session.Session(\n",
    "        aws_access_key_id=CONFIG[\"aws_key\"], aws_secret_access_key=CONFIG[\"aws_secret\"]\n",
    "    )\n",
    "    S3ArtifactRepository._get_s3_client = lambda self: session.client(\n",
    "        \"s3\", endpoint_url=CONFIG[\"s3_endpoint\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# === CLIENTS ===\n",
    "redis_client = redis.Redis(\n",
    "    host=CONFIG[\"redis_host\"],\n",
    "    port=CONFIG[\"redis_port\"],\n",
    "    db=CONFIG[\"redis_db\"],\n",
    "    decode_responses=True,\n",
    "    password=\"123456\",\n",
    ")\n",
    "qdrant_client = QdrantClient(url=CONFIG[\"qdrant_url\"])\n",
    "\n",
    "\n",
    "# === SimpleIDMapper ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index = mapping[\"item_to_index\"]\n",
    "        # Convert list to dict: index (int) -> item_id (str)\n",
    "        self.index_to_item = {\n",
    "            i: item_id for i, item_id in enumerate(mapping[\"index_to_item\"])\n",
    "        }\n",
    "\n",
    "\n",
    "# === Load TorchScript model + IDMapper ===\n",
    "def load_model():\n",
    "    configure_mlflow()\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{CONFIG['mlf_model_name']}'\")\n",
    "    champs = [v for v in versions if v.tags.get(\"champion\", \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise RuntimeError(\n",
    "            f\"‚ùå No champion version found for model '{CONFIG['mlf_model_name']}'\"\n",
    "        )\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model = mlflow.pytorch.load_model(\n",
    "        f\"models:/{CONFIG['mlf_model_name']}/{champ.version}\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"‚úÖ Loaded model v{champ.version} (run_id={run_id}) and IDMapper.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "\n",
    "# === Inference Utilities ===\n",
    "def get_item_embedding(\n",
    "    model, id_mapper, item_id, device=torch.device(\"cpu\")\n",
    ") -> torch.Tensor:\n",
    "    idx = torch.tensor([id_mapper.item_to_index[item_id]], device=device)\n",
    "    return model.embeddings(idx).squeeze(0)\n",
    "\n",
    "\n",
    "def get_topk_similar(\n",
    "    model, id_mapper, item_id, top_k=10, device=torch.device(\"cpu\")\n",
    ") -> list:\n",
    "    target_idx = id_mapper.item_to_index[item_id]\n",
    "    weight = model.embeddings.weight[:-1].to(device)  # exclude padding_idx\n",
    "    with torch.no_grad():\n",
    "        target_emb = weight[target_idx]\n",
    "        sims = torch.matmul(weight, target_emb)\n",
    "    sims[target_idx] = -np.inf  # exclude self\n",
    "    topk = torch.topk(sims, k=top_k)\n",
    "    return [\n",
    "        (id_mapper.index_to_item[int(i)], float(s))\n",
    "        for i, s in zip(topk.indices, topk.values)\n",
    "    ]\n",
    "\n",
    "\n",
    "# === Compute Recommendations ===\n",
    "def compute_recommendations():\n",
    "    model, id_mapper = load_model()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    all_indices = list(id_mapper.index_to_item.keys())\n",
    "\n",
    "    # Load all embeddings from Qdrant\n",
    "    records = qdrant_client.retrieve(\n",
    "        CONFIG[\"qdrant_collection_name\"], ids=all_indices, with_vectors=True\n",
    "    )\n",
    "    id_to_vec = {rec.id: rec.vector for rec in records if rec.vector is not None}\n",
    "\n",
    "    recs = []\n",
    "\n",
    "    for i in tqdm(range(0, len(all_indices), CONFIG[\"batch_size\"]), desc=\"Processing\"):\n",
    "        batch_indices = all_indices[i : i + CONFIG[\"batch_size\"]]\n",
    "        batch_vectors = [id_to_vec.get(idx) for idx in batch_indices]\n",
    "\n",
    "        batch_neighbors = []\n",
    "        for idx, vec in zip(batch_indices, batch_vectors):\n",
    "            if vec is None:\n",
    "                batch_neighbors.append([])\n",
    "                continue\n",
    "            neighbors = qdrant_client.search(\n",
    "                collection_name=CONFIG[\"qdrant_collection_name\"],\n",
    "                query_vector=vec,\n",
    "                limit=CONFIG[\"top_K\"] + 1,\n",
    "            )\n",
    "            # Remove self from results\n",
    "            neighbor_ids = [n.id for n in neighbors if n.id != idx][: CONFIG[\"top_K\"]]\n",
    "            batch_neighbors.append(neighbor_ids)\n",
    "\n",
    "        batch_scores = []\n",
    "        for idx, neighbors in zip(batch_indices, batch_neighbors):\n",
    "            if not neighbors:\n",
    "                batch_scores.append([])\n",
    "                continue\n",
    "\n",
    "            target_id = id_mapper.index_to_item[idx]\n",
    "            neighbor_ids = [id_mapper.index_to_item[nid] for nid in neighbors]\n",
    "\n",
    "            # Compute scores using model\n",
    "            sample_input = {\n",
    "                \"target_items\": [target_id] * len(neighbor_ids),\n",
    "                \"context_items\": neighbor_ids,\n",
    "            }\n",
    "            try:\n",
    "                scores = predict_batch(model, id_mapper, sample_input, device)\n",
    "                batch_scores.append(scores.tolist())\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Prediction failed for {target_id}: {e}\")\n",
    "                batch_scores.append([0.0] * len(neighbor_ids))\n",
    "\n",
    "        # Write to Redis and JSONL\n",
    "        for idx, neighbors, scores in zip(batch_indices, batch_neighbors, batch_scores):\n",
    "            if not neighbors:\n",
    "                continue\n",
    "            sorted_pairs = sorted(\n",
    "                zip(neighbors, scores), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            top_neighbors, top_scores = zip(*sorted_pairs[: CONFIG[\"top_k\"]])\n",
    "            neighbor_ids = [id_mapper.index_to_item[n] for n in top_neighbors]\n",
    "            target_id = id_mapper.index_to_item[idx]\n",
    "\n",
    "            rec = {\n",
    "                \"target_item\": target_id,\n",
    "                \"rec_item_ids\": neighbor_ids,\n",
    "                \"rec_scores\": list(top_scores),\n",
    "            }\n",
    "\n",
    "            recs.append(rec)\n",
    "            redis_client.set(f\"rec:{target_id}\", json.dumps(rec))\n",
    "\n",
    "    os.makedirs(os.path.dirname(CONFIG[\"output_file\"]), exist_ok=True)\n",
    "    with open(CONFIG[\"output_file\"], \"w\") as f:\n",
    "        for rec in recs:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(\n",
    "        f\"‚úÖ Completed. Stored {len(recs)} recommendations to Redis and {CONFIG['output_file']}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# === Inference Utility for Batch Prediction ===\n",
    "def predict_batch(\n",
    "    model, id_mapper, batch: dict, device=torch.device(\"cpu\")\n",
    ") -> np.ndarray:\n",
    "    tgt = torch.tensor(\n",
    "        [id_mapper.item_to_index[i] for i in batch[\"target_items\"]], device=device\n",
    "    )\n",
    "    ctx = torch.tensor(\n",
    "        [id_mapper.item_to_index[i] for i in batch[\"context_items\"]], device=device\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        return model(tgt, ctx).cpu().numpy()\n",
    "\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    compute_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b2d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Recommendation found:\n",
      "{\n",
      "  \"target_item\": \"B0002YV94U\",\n",
      "  \"rec_item_ids\": [\n",
      "    \"B087GTFML5\",\n",
      "    \"B087GY3RJS\",\n",
      "    \"B08KXGHX9V\",\n",
      "    \"B00K8A08YU\",\n",
      "    \"B0C3GL76CN\",\n",
      "    \"B00DW1JT3I\",\n",
      "    \"B00K5OLKCI\",\n",
      "    \"B00U5MVOX0\",\n",
      "    \"B0CH269DQ5\",\n",
      "    \"B00LDX3OFQ\"\n",
      "  ],\n",
      "  \"rec_scores\": [\n",
      "    0.6255476474761963,\n",
      "    0.6213389039039612,\n",
      "    0.616352379322052,\n",
      "    0.6159809231758118,\n",
      "    0.6147825121879578,\n",
      "    0.6140996813774109,\n",
      "    0.6071546673774719,\n",
      "    0.603735089302063,\n",
      "    0.6013337969779968,\n",
      "    0.6004250645637512\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "# C·∫•u h√¨nh Redis (ƒë·∫£m b·∫£o gi·ªëng CONFIG trong script g·ªëc)\n",
    "redis_client = redis.Redis(\n",
    "    host=\"localhost\",  # Ho·∫∑c thay b·∫±ng gi√° tr·ªã t·ª´ CONFIG[\"redis_host\"]\n",
    "    port=6379,  # CONFIG[\"redis_port\"]\n",
    "    db=0,  # CONFIG[\"redis_db\"]\n",
    "    decode_responses=True,  # ƒê·∫£m b·∫£o tr·∫£ v·ªÅ string thay v√¨ bytes\n",
    "    password=\"123456\",  # N·∫øu c√≥ auth\n",
    ")\n",
    "\n",
    "# Item ID c·∫ßn truy v·∫•n\n",
    "item_id = \"B0002YV94U\"\n",
    "key = f\"rec:{item_id}\"\n",
    "\n",
    "# L·∫•y d·ªØ li·ªáu t·ª´ Redis\n",
    "rec_json = redis_client.get(key)\n",
    "\n",
    "if rec_json:\n",
    "    rec_data = json.loads(rec_json)\n",
    "    print(\"‚úÖ Recommendation found:\")\n",
    "    print(json.dumps(rec_data, indent=2))\n",
    "else:\n",
    "    print(f\"‚ùå No recommendation found for item {item_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db2bc6",
   "metadata": {},
   "source": [
    "## Load popular item to Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f932e6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) connection to server at \"simulate-oltp-db.cdkwg6wyo7r8.ap-southeast-1.rds.amazonaws.com\" (52.220.25.32), port 5432 failed: FATAL:  database \"raw_data\" does not exist\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/base.py:3297\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3276\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3277\u001b[39m \n\u001b[32m   3278\u001b[39m \u001b[33;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3295\u001b[39m \n\u001b[32m   3296\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:449\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    443\u001b[39m \n\u001b[32m    444\u001b[39m \u001b[33;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \n\u001b[32m    448\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:713\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/impl.py:179\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:390\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:675\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:901\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:897\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    896\u001b[39m \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/create.py:646\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    644\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/default.py:625\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    624\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"simulate-oltp-db.cdkwg6wyo7r8.ap-southeast-1.rds.amazonaws.com\" (52.220.25.32), port 5432 failed: FATAL:  database \"raw_data\" does not exist\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     25\u001b[39m engine = create_engine(conn_str)\n\u001b[32m     26\u001b[39m query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[33m    SELECT \u001b[39m\n\u001b[32m     28\u001b[39m \u001b[33m        parent_asin,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m \u001b[33m    WHERE parent_asin IS NOT NULL\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 2. T√≠nh to√°n th·ªëng k√™\u001b[39;00m\n\u001b[32m     36\u001b[39m agg_df = (\n\u001b[32m     37\u001b[39m     df.groupby(\u001b[33m\"\u001b[39m\u001b[33mparent_asin\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m     .agg(rating_count=(\u001b[33m\"\u001b[39m\u001b[33mrating\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m), rating_avg=(\u001b[33m\"\u001b[39m\u001b[33mrating\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     39\u001b[39m     .reset_index()\n\u001b[32m     40\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/pandas/io/sql.py:706\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    703\u001b[39m     dtype_backend = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpandasSQL_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[32m    708\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql.read_query(\n\u001b[32m    709\u001b[39m             sql,\n\u001b[32m    710\u001b[39m             index_col=index_col,\n\u001b[32m   (...)\u001b[39m\u001b[32m    716\u001b[39m             dtype=dtype,\n\u001b[32m    717\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/pandas/io/sql.py:908\u001b[39m, in \u001b[36mpandasSQL_builder\u001b[39m\u001b[34m(con, schema, need_transaction)\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUsing URI string without sqlalchemy installed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sqlalchemy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(con, (\u001b[38;5;28mstr\u001b[39m, sqlalchemy.engine.Connectable)):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSQLDatabase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_transaction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m adbc = import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33madbc_driver_manager.dbapi\u001b[39m\u001b[33m\"\u001b[39m, errors=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adbc \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(con, adbc.Connection):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/pandas/io/sql.py:1648\u001b[39m, in \u001b[36mSQLDatabase.__init__\u001b[39m\u001b[34m(self, con, schema, need_transaction)\u001b[39m\n\u001b[32m   1646\u001b[39m     \u001b[38;5;28mself\u001b[39m.exit_stack.callback(con.dispose)\n\u001b[32m   1647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(con, Engine):\n\u001b[32m-> \u001b[39m\u001b[32m1648\u001b[39m     con = \u001b[38;5;28mself\u001b[39m.exit_stack.enter_context(\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m need_transaction \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m con.in_transaction():\n\u001b[32m   1650\u001b[39m     \u001b[38;5;28mself\u001b[39m.exit_stack.enter_context(con.begin())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/base.py:3273\u001b[39m, in \u001b[36mEngine.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3250\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Connection:\n\u001b[32m   3251\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[32m   3252\u001b[39m \n\u001b[32m   3253\u001b[39m \u001b[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3270\u001b[39m \n\u001b[32m   3271\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/base.py:147\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = engine.raw_connection()\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         \u001b[43mConnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2436\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception_noconnection\u001b[39m\u001b[34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[39m\n\u001b[32m   2434\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2435\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2437\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2438\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    147\u001b[39m         Connection._handle_dbapi_exception_noconnection(\n\u001b[32m    148\u001b[39m             err, dialect, engine\n\u001b[32m    149\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/base.py:3297\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m   3276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3277\u001b[39m \n\u001b[32m   3278\u001b[39m \u001b[33;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3295\u001b[39m \n\u001b[32m   3296\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:449\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    443\u001b[39m \n\u001b[32m    444\u001b[39m \u001b[33;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \n\u001b[32m    448\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1256\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_checkout\u001b[39m(\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1261\u001b[39m     fairy: Optional[_ConnectionFairy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1262\u001b[39m ) -> _ConnectionFairy:\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m         fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1267\u001b[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:713\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    711\u001b[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    716\u001b[39m     dbapi_connection = rec.get_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/impl.py:179\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_connection()\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inc_overflow():\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m util.safe_reraise():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:390\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ConnectionPoolEntry:\n\u001b[32m    388\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:675\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28mself\u001b[39m.__pool = pool\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:901\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/pool/base.py:897\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m     pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/create.py:646\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    643\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    644\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/sqlalchemy/engine/default.py:625\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    624\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pipeline-caching/lib/python3.11/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: (psycopg2.OperationalError) connection to server at \"simulate-oltp-db.cdkwg6wyo7r8.ap-southeast-1.rds.amazonaws.com\" (52.220.25.32), port 5432 failed: FATAL:  database \"raw_data\" does not exist\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import redis\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = os.path.abspath(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "POSTGRES_URI = os.environ[\"POSTGRES_URI_OLTP\"]\n",
    "conn_str = POSTGRES_URI\n",
    "table = \"public.reviews\"\n",
    "\n",
    "# Redis config\n",
    "redis_client = redis.Redis(\n",
    "    host=\"localhost\", port=6379, db=0, decode_responses=True, password=\"123456\"\n",
    ")\n",
    "\n",
    "# 1. Load t·ª´ PostgreSQL\n",
    "engine = create_engine(conn_str)\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        parent_asin,\n",
    "        rating\n",
    "    FROM {table}\n",
    "    WHERE parent_asin IS NOT NULL\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# 2. T√≠nh to√°n th·ªëng k√™\n",
    "agg_df = (\n",
    "    df.groupby(\"parent_asin\")\n",
    "    .agg(rating_count=(\"rating\", \"count\"), rating_avg=(\"rating\", \"mean\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 3. T√≠nh popularity score\n",
    "agg_df[\"score\"] = agg_df[\"rating_count\"] * (agg_df[\"rating_avg\"] - 3.0)\n",
    "\n",
    "# 4. L·∫•y top ph·ªï bi·∫øn theo score\n",
    "TOP_K = 500\n",
    "top_df = agg_df.sort_values(\"score\", ascending=False).head(TOP_K)\n",
    "\n",
    "# ‚úÖ In ra ƒë·ªÉ ki·ªÉm tra tr∆∞·ªõc khi l∆∞u\n",
    "print(\"üîç Top popular parent_asin (score-based):\")\n",
    "print(top_df[[\"parent_asin\", \"rating_count\", \"rating_avg\", \"score\"]])\n",
    "\n",
    "# 5. Ghi v√†o Redis n·∫øu OK\n",
    "redis_key = \"popular_parent_asin_score\"\n",
    "redis_client.delete(redis_key)\n",
    "\n",
    "for _, row in top_df.iterrows():\n",
    "    redis_client.zadd(redis_key, {row[\"parent_asin\"]: float(row[\"score\"])})\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ l∆∞u {len(top_df)} popular parent_asin v√†o Redis key: {redis_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Top 10 popular parent_asin from Redis:\n",
      " 1. ASIN: B0BW3QTWJJ | Score: 780.00\n",
      " 2. ASIN: B07C4NGT17 | Score: 422.00\n",
      " 3. ASIN: B0054TRQA4 | Score: 378.00\n",
      " 4. ASIN: B00D8STBHY | Score: 367.00\n",
      " 5. ASIN: B09QPXVW35 | Score: 313.00\n",
      " 6. ASIN: B07N29HQMN | Score: 257.00\n",
      " 7. ASIN: B00FZMDAO6 | Score: 248.00\n",
      " 8. ASIN: B0BG94QRLZ | Score: 241.00\n",
      " 9. ASIN: B09PH8LV57 | Score: 238.00\n",
      "10. ASIN: B004S8F7QM | Score: 238.00\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "\n",
    "# K·∫øt n·ªëi Redis\n",
    "redis_client = redis.Redis(\n",
    "    host=\"localhost\", port=6379, db=0, decode_responses=True, password=\"123456\"\n",
    ")\n",
    "\n",
    "# Redis key b·∫°n ƒë√£ l∆∞u tr∆∞·ªõc ƒë√≥\n",
    "redis_key = \"popular_parent_asin_score\"\n",
    "\n",
    "# Truy v·∫•n top 10 ph·ªï bi·∫øn nh·∫•t (score cao nh·∫•t)\n",
    "top_items = redis_client.zrevrange(redis_key, 0, 9, withscores=True)\n",
    "\n",
    "# In ra k·∫øt qu·∫£\n",
    "print(\"üî• Top 10 popular parent_asin from Redis:\")\n",
    "for rank, (asin, score) in enumerate(top_items, start=1):\n",
    "    print(f\"{rank:2d}. ASIN: {asin} | Score: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline-caching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
