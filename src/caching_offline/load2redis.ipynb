{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b912f8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2d723e1f8743b9a4d04569eaede57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/29 08:56:40 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.7.1+cu126'\n",
      "/home/duong/Documents/datn1/.venv/lib/python3.11/site-packages/torch/serialization.py:1488: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4da58d4dd44a90aa26226902284c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model v1 (run_id=aac89aa42a3c447eac1294546df933e6) and IDMapper.\n",
      "\n",
      "🔹 Embedding for 'B00000IV95' (index 36): [ 0.05149313  0.0443965  -0.01442701  0.07874709  0.07239346]\n",
      "\n",
      "🔹 Top 5 similar items to 'B00000IV95':\n",
      " - B0C3H818H4 (index 4009): score = 2.2501\n",
      " - B0C48KPLZ2 (index 4026): score = 1.6529\n",
      " - 0975277324 (index 5): score = 1.6294\n",
      " - B0C1FX3BGK (index 3976): score = 1.5719\n",
      " - B00000IV35 (index 35): score = 1.5157\n",
      "\n",
      "🔹 Batch prediction scores:\n",
      " - (B00000IV95, B0792X1RSC): score = 0.5309\n",
      " - (B0792X1RSC, B00000IV95): score = 0.5309\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "\n",
    "# === Cấu hình MLflow + MinIO ===\n",
    "def configure_mlflow(s3_endpoint: str, aws_key: str, aws_secret: str, tracking_uri: str):\n",
    "    os.environ.update({\n",
    "        \"AWS_ACCESS_KEY_ID\": aws_key,\n",
    "        \"AWS_SECRET_ACCESS_KEY\": aws_secret,\n",
    "        \"MLFLOW_S3_ENDPOINT_URL\": s3_endpoint,\n",
    "        \"MLFLOW_S3_IGNORE_TLS\": \"true\"\n",
    "    })\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "    session = boto3.session.Session(\n",
    "        aws_access_key_id=aws_key,\n",
    "        aws_secret_access_key=aws_secret\n",
    "    )\n",
    "    S3ArtifactRepository._get_s3_client = lambda self: session.client(\"s3\", endpoint_url=s3_endpoint)\n",
    "\n",
    "# === IDMapper đơn giản ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index: Dict[str, int] = mapping[\"item_to_index\"]\n",
    "        self.index_to_item: Dict[int, str] = mapping[\"index_to_item\"]\n",
    "\n",
    "# === Load TorchScript model + IDMapper từ registered model ===\n",
    "def load_champion_model(model_name: str) -> Tuple[torch.jit.ScriptModule, SimpleIDMapper]:\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    champs = [v for v in versions if v.tags.get(\"champion\", \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise RuntimeError(f\"❌ No champion version found for model '{model_name}'\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model = mlflow.pytorch.load_model(f\"models:/{model_name}/{champ.version}\")\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    print(f\"✅ Loaded model v{champ.version} (run_id={run_id}) and IDMapper.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "# === Inference Utilities ===\n",
    "def get_item_embedding(model, id_mapper, item_id, device=torch.device(\"cpu\")) -> torch.Tensor:\n",
    "    idx = torch.tensor([id_mapper.item_to_index[item_id]], device=device)\n",
    "    return model.embeddings(idx).squeeze(0)\n",
    "\n",
    "def get_topk_similar(model, id_mapper, item_id, top_k=10, device=torch.device(\"cpu\")) -> List[Tuple[str, int, float]]:\n",
    "    target_idx = id_mapper.item_to_index[item_id]\n",
    "    weight = model.embeddings.weight[:-1].to(device)  # exclude padding_idx\n",
    "    with torch.no_grad():\n",
    "        target_emb = weight[target_idx]\n",
    "        sims = torch.matmul(weight, target_emb)\n",
    "    sims[target_idx] = -np.inf  # exclude self\n",
    "    topk = torch.topk(sims, k=top_k)\n",
    "    return [\n",
    "        (id_mapper.index_to_item[int(i)], int(i), float(s))\n",
    "        for i, s in zip(topk.indices, topk.values)\n",
    "    ]\n",
    "\n",
    "def predict_batch(model, id_mapper, batch: Dict[str, List[str]], device=torch.device(\"cpu\")) -> np.ndarray:\n",
    "    tgt = torch.tensor([id_mapper.item_to_index[i] for i in batch[\"target_items\"]], device=device)\n",
    "    ctx = torch.tensor([id_mapper.item_to_index[i] for i in batch[\"context_items\"]], device=device)\n",
    "    with torch.no_grad():\n",
    "        return model(tgt, ctx).cpu().numpy()\n",
    "\n",
    "# === Chạy thử ===\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Cấu hình\n",
    "    configure_mlflow(\n",
    "        s3_endpoint=\"http://127.0.0.1:9010\",\n",
    "        aws_key=\"admin\",\n",
    "        aws_secret=\"Password1234\",\n",
    "        tracking_uri=\"http://localhost:5002\"\n",
    "    )\n",
    "\n",
    "    # 2. Load model + IDMapper\n",
    "    model_name = \"item2vec_skipgram\"\n",
    "    model, id_mapper = load_champion_model(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 3. In embedding\n",
    "    test_item = \"B00000IV95\"\n",
    "    test_idx = id_mapper.item_to_index[test_item]\n",
    "    emb = get_item_embedding(model, id_mapper, test_item, device)\n",
    "    print(f\"\\n🔹 Embedding for '{test_item}' (index {test_idx}): {emb[:5].detach().cpu().numpy()}\")\n",
    "\n",
    "    # 4. Tìm top K tương tự\n",
    "    topk = get_topk_similar(model, id_mapper, test_item, top_k=5, device=device)\n",
    "    print(f\"\\n🔹 Top 5 similar items to '{test_item}':\")\n",
    "    for iid, idx, score in topk:\n",
    "        print(f\" - {iid} (index {idx}): score = {score:.4f}\")\n",
    "\n",
    "    # 5. Dự đoán batch\n",
    "    batch = {\n",
    "        \"target_items\": [test_item, \"B0792X1RSC\"],\n",
    "        \"context_items\": [\"B0792X1RSC\", test_item]\n",
    "    }\n",
    "    scores = predict_batch(model, id_mapper, batch, device)\n",
    "    print(f\"\\n🔹 Batch prediction scores:\")\n",
    "    for t, c, s in zip(batch[\"target_items\"], batch[\"context_items\"], scores):\n",
    "        print(f\" - ({t}, {c}): score = {s:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941bd42",
   "metadata": {},
   "source": [
    "## Load embedding to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b76cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting embedding indexing...\n",
      "📦 Loading model from URI: models:/item2vec_skipgram/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41322d49b61f4234bd606033e492a1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/29 09:00:35 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.7.1+cu126'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226f79464b7e446bb95adc1ffc83bd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model version 1 (run_id=aac89aa42a3c447eac1294546df933e6) with 4143 items.\n",
      "✅ Extracted embeddings with shape (4143, 256)\n",
      "🌐 Connecting to Qdrant at http://localhost:6333\n",
      "✅ Created new collection 'item2vec_collection' with dim=256\n",
      "✅ Upserted 4143 embeddings into Qdrant collection 'item2vec_collection'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "import boto3\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "PROJECT_ROOT = \"/home/duong/Documents/datn1/src/model_item2vec\"\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MLflow config\n",
    "S3_ENDPOINT = \"http://127.0.0.1:9010\"\n",
    "AWS_KEY = \"admin\"\n",
    "AWS_SECRET = \"Password1234\"\n",
    "TRACKING_URI = \"http://localhost:5002\"\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_KEY\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = S3_ENDPOINT\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "# Configure boto3 for MinIO\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=AWS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET\n",
    ")\n",
    "S3ArtifactRepository._get_s3_client = lambda self: session.client(\"s3\", endpoint_url=S3_ENDPOINT)\n",
    "\n",
    "# Qdrant config\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "QDRANT_COLLECTION = os.getenv(\"QDRANT_COLLECTION_NAME\", \"item2vec_collection\")\n",
    "\n",
    "MODEL_NAME = \"item2vec_skipgram\"\n",
    "TAG_NAME = \"champion\"\n",
    "\n",
    "# === SimpleIDMapper ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index = mapping[\"item_to_index\"]\n",
    "        self.index_to_item = mapping[\"index_to_item\"]\n",
    "\n",
    "# === STEP 1: LOAD MODEL AND ID MAPPING ===\n",
    "def load_model_from_mlflow(model_name: str, tag: str = \"champion\") -> tuple:\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    champs = [v for v in versions if v.tags.get(tag, \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise ValueError(f\"❌ No version tagged '{tag}' found for model '{model_name}'.\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model_uri = f\"models:/{model_name}/{champ.version}\"\n",
    "    print(f\"📦 Loading model from URI: {model_uri}\")\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    print(f\"✅ Loaded model version {champ.version} (run_id={run_id}) with {len(id_mapper.item_to_index)} items.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "# === STEP 2: EXTRACT EMBEDDINGS ===\n",
    "def get_all_embeddings(model, id_mapper: SimpleIDMapper) -> tuple:\n",
    "    item_ids = list(id_mapper.item_to_index.keys())\n",
    "    item_indices = [id_mapper.item_to_index[id_] for id_ in item_ids]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    tensor_indices = torch.tensor(item_indices, device=device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.embeddings(tensor_indices).detach().cpu().numpy()\n",
    "    print(f\"✅ Extracted embeddings with shape {embeddings.shape}\")\n",
    "    return item_ids, embeddings\n",
    "\n",
    "# === STEP 3: INDEX TO QDRANT (delete and recreate collection) ===\n",
    "def index_embeddings_to_qdrant(item_ids: list, embeddings: np.ndarray, qdrant_url: str, collection_name: str):\n",
    "    try:\n",
    "        client = QdrantClient(url=qdrant_url)\n",
    "        print(f\"🌐 Connecting to Qdrant at {qdrant_url}\")\n",
    "\n",
    "        vector_dim = embeddings.shape[1]\n",
    "\n",
    "        # Delete collection if it exists\n",
    "        if client.collection_exists(collection_name):\n",
    "            client.delete_collection(collection_name)\n",
    "            print(f\"🗑️ Deleted existing collection '{collection_name}'.\")\n",
    "\n",
    "        # Create new collection\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_dim, distance=Distance.COSINE),\n",
    "        )\n",
    "        print(f\"✅ Created new collection '{collection_name}' with dim={vector_dim}\")\n",
    "\n",
    "        # Prepare points using sequential IDs\n",
    "        points = [\n",
    "            PointStruct(id=idx, vector=vec.tolist(), payload={\"item_id\": item_id})\n",
    "            for idx, (item_id, vec) in enumerate(zip(item_ids, embeddings))\n",
    "        ]\n",
    "\n",
    "        # Upsert points\n",
    "        upsert_result = client.upsert(collection_name=collection_name, points=points)\n",
    "        assert str(upsert_result.status) == \"completed\"\n",
    "        print(f\"✅ Upserted {len(points)} embeddings into Qdrant collection '{collection_name}'\")\n",
    "\n",
    "    except UnexpectedResponse as e:\n",
    "        print(\"❌ Qdrant returned unexpected response (possibly 404 or 400).\")\n",
    "        print(\"➡️ Check that the endpoint is correct and Qdrant is running.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(\"❌ Failed to index embeddings to Qdrant.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    print(\"🚀 Starting embedding indexing...\")\n",
    "    model, id_mapper = load_model_from_mlflow(MODEL_NAME, TAG_NAME)\n",
    "    item_ids, embeddings = get_all_embeddings(model, id_mapper)\n",
    "    index_embeddings_to_qdrant(item_ids, embeddings, QDRANT_URL, QDRANT_COLLECTION)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489b078",
   "metadata": {},
   "source": [
    "### Test Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5fd1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting similarity search...\n",
      "📦 Loading model from URI: models:/item2vec_skipgram/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ec8f1e68b3439e9cdbc5e4fe4b345d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/29 09:00:46 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.7.1+cu126'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c922ce11124e6093d88afb7615a464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model version 1 (run_id=aac89aa42a3c447eac1294546df933e6) with 4143 items.\n",
      "✅ Generated embedding for 'B00EMGM1JQ' with shape (256,)\n",
      "🌐 Connecting to Qdrant at http://localhost:6333\n",
      "\n",
      "🔍 Top 5 items similar to 'B00EMGM1JQ':\n",
      "🟢 B00EMGM1JQ      | Score: 1.0000\n",
      "🟢 B00C6Q4HEQ      | Score: 0.7464\n",
      "🟢 B00C6Q5S44      | Score: 0.6372\n",
      "🟢 B00C6Q1Z6E      | Score: 0.6326\n",
      "🟢 B00CQHZ2LW      | Score: 0.6079\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import mlflow\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "import boto3\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "QDRANT_COLLECTION = \"item2vec_collection\"\n",
    "MODEL_NAME = \"item2vec_skipgram\"\n",
    "TAG_NAME = \"champion\"\n",
    "ITEM_ID = \"B00EMGM1JQ\"\n",
    "TOP_K = 5\n",
    "\n",
    "S3_ENDPOINT = \"http://127.0.0.1:9010\"\n",
    "AWS_KEY = \"admin\"\n",
    "AWS_SECRET = \"Password1234\"\n",
    "TRACKING_URI = \"http://localhost:5002\"\n",
    "\n",
    "# Configure MLflow and MinIO\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_KEY\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = S3_ENDPOINT\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=AWS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET\n",
    ")\n",
    "S3ArtifactRepository._get_s3_client = lambda self: session.client(\"s3\", endpoint_url=S3_ENDPOINT)\n",
    "\n",
    "# === SimpleIDMapper ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index = mapping[\"item_to_index\"]\n",
    "        self.index_to_item = mapping[\"index_to_item\"]\n",
    "\n",
    "# === STEP 1: Load model and ID mapping ===\n",
    "def load_model_and_mapping(model_name: str, tag: str = \"champion\") -> tuple:\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    champs = [v for v in versions if v.tags.get(tag, \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise ValueError(f\"❌ No version tagged '{tag}' found for model '{model_name}'.\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model_uri = f\"models:/{model_name}/{champ.version}\"\n",
    "    print(f\"📦 Loading model from URI: {model_uri}\")\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    print(f\"✅ Loaded model version {champ.version} (run_id={run_id}) with {len(id_mapper.item_to_index)} items.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "# === STEP 2: Get embedding vector for ITEM_ID ===\n",
    "def get_item_embedding(model, id_mapper: SimpleIDMapper, item_id: str, device: torch.device) -> np.ndarray:\n",
    "    idx = id_mapper.item_to_index.get(item_id)\n",
    "    if idx is None:\n",
    "        raise ValueError(f\"❌ Item ID '{item_id}' not found in ID mapping\")\n",
    "    tensor_idx = torch.tensor([idx], device=device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.embeddings(tensor_idx).detach().cpu().numpy()[0]\n",
    "    return embedding\n",
    "\n",
    "# === STEP 3: Query Qdrant with embedding vector ===\n",
    "def search_similar_items(qdrant_url: str, collection_name: str, embedding: np.ndarray, top_k: int) -> list:\n",
    "    try:\n",
    "        qdrant = QdrantClient(url=qdrant_url)\n",
    "        print(f\"🌐 Connecting to Qdrant at {qdrant_url}\")\n",
    "        if not qdrant.collection_exists(collection_name):\n",
    "            raise ValueError(f\"❌ Collection '{collection_name}' does not exist in Qdrant\")\n",
    "        \n",
    "        results = qdrant.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=embedding.tolist(),\n",
    "            limit=top_k,\n",
    "        )\n",
    "        return results\n",
    "    except UnexpectedResponse as e:\n",
    "        print(\"❌ Qdrant returned unexpected response (possibly 404 or 400).\")\n",
    "        print(\"➡️ Check that the endpoint is correct and Qdrant is running.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(\"❌ Failed to query Qdrant.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    print(\"🚀 Starting similarity search...\")\n",
    "    # Load model and ID mapping\n",
    "    model, id_mapper = load_model_and_mapping(MODEL_NAME, TAG_NAME)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get embedding for ITEM_ID\n",
    "    embedding = get_item_embedding(model, id_mapper, ITEM_ID, device)\n",
    "    print(f\"✅ Generated embedding for '{ITEM_ID}' with shape {embedding.shape}\")\n",
    "    \n",
    "    # Query Qdrant\n",
    "    results = search_similar_items(QDRANT_URL, QDRANT_COLLECTION, embedding, TOP_K)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n🔍 Top {TOP_K} items similar to '{ITEM_ID}':\")\n",
    "    for r in results:\n",
    "        item = r.payload.get(\"item_id\", \"<missing>\")\n",
    "        print(f\"🟢 {item:15} | Score: {r.score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f64b16",
   "metadata": {},
   "source": [
    "## Load pre-recommend to Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e711ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80a7749a6584990ae8c454ab54b68d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/29 09:04:04 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.7.1+cu126'\n",
      "/home/duong/Documents/datn1/.venv/lib/python3.11/site-packages/torch/serialization.py:1488: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f77416c2ed04c9eb7f97e4c4d4a7907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model v1 (run_id=aac89aa42a3c447eac1294546df933e6) and IDMapper.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd8c194e8de49068f3ca41fb562ef96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed. Stored 4143 recommendations to Redis and ../../data/batch_recs.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import redis\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import sys\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm.auto import tqdm\n",
    "import boto3\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "PROJECT_ROOT = \"/home/duong/Documents/datn1/src/model_item2vec\"\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "CONFIG = {\n",
    "    \"mlf_model_name\": \"item2vec_skipgram\",\n",
    "    \"qdrant_url\": f\"{os.getenv('QDRANT_HOST', 'localhost')}:{os.getenv('QDRANT_PORT', '6333')}\",\n",
    "    \"qdrant_collection_name\": os.getenv(\"QDRANT_COLLECTION_NAME\", \"item2vec_collection\"),\n",
    "    \"redis_host\": os.getenv(\"REDIS_HOST\", \"localhost\"),\n",
    "    \"redis_port\": 6379,\n",
    "    \"redis_db\": int(os.getenv(\"REDIS_DB\", 0)),\n",
    "    \"batch_size\": 256,\n",
    "    \"top_k\": 10,\n",
    "    \"top_K\": 100,\n",
    "    \"output_file\": \"../../data/batch_recs.jsonl\",\n",
    "    \"s3_endpoint\": \"http://127.0.0.1:9010\",\n",
    "    \"aws_key\": \"admin\",\n",
    "    \"aws_secret\": \"Password1234\"\n",
    "}\n",
    "\n",
    "# === ENV SETUP ===\n",
    "def configure_mlflow():\n",
    "    os.environ.update({\n",
    "        \"AWS_ACCESS_KEY_ID\": CONFIG[\"aws_key\"],\n",
    "        \"AWS_SECRET_ACCESS_KEY\": CONFIG[\"aws_secret\"],\n",
    "        \"MLFLOW_S3_ENDPOINT_URL\": CONFIG[\"s3_endpoint\"],\n",
    "        \"MLFLOW_S3_IGNORE_TLS\": \"true\"\n",
    "    })\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5002\")\n",
    "\n",
    "    session = boto3.session.Session(\n",
    "        aws_access_key_id=CONFIG[\"aws_key\"],\n",
    "        aws_secret_access_key=CONFIG[\"aws_secret\"]\n",
    "    )\n",
    "    S3ArtifactRepository._get_s3_client = lambda self: session.client(\"s3\", endpoint_url=CONFIG[\"s3_endpoint\"])\n",
    "\n",
    "# === CLIENTS ===\n",
    "redis_client = redis.Redis(\n",
    "    host=CONFIG[\"redis_host\"],\n",
    "    port=CONFIG[\"redis_port\"],\n",
    "    db=CONFIG[\"redis_db\"],\n",
    "    decode_responses=True,\n",
    "    password=\"123456\"\n",
    ")\n",
    "qdrant_client = QdrantClient(url=CONFIG[\"qdrant_url\"])\n",
    "\n",
    "# === SimpleIDMapper ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index = mapping[\"item_to_index\"]\n",
    "        # Convert list to dict: index (int) -> item_id (str)\n",
    "        self.index_to_item = {i: item_id for i, item_id in enumerate(mapping[\"index_to_item\"])}\n",
    "# === Load TorchScript model + IDMapper ===\n",
    "def load_model():\n",
    "    configure_mlflow()\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{CONFIG['mlf_model_name']}'\")\n",
    "    champs = [v for v in versions if v.tags.get(\"champion\", \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise RuntimeError(f\"❌ No champion version found for model '{CONFIG['mlf_model_name']}'\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model = mlflow.pytorch.load_model(f\"models:/{CONFIG['mlf_model_name']}/{champ.version}\")\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"✅ Loaded model v{champ.version} (run_id={run_id}) and IDMapper.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "# === Inference Utilities ===\n",
    "def get_item_embedding(model, id_mapper, item_id, device=torch.device(\"cpu\")) -> torch.Tensor:\n",
    "    idx = torch.tensor([id_mapper.item_to_index[item_id]], device=device)\n",
    "    return model.embeddings(idx).squeeze(0)\n",
    "\n",
    "def get_topk_similar(model, id_mapper, item_id, top_k=10, device=torch.device(\"cpu\")) -> list:\n",
    "    target_idx = id_mapper.item_to_index[item_id]\n",
    "    weight = model.embeddings.weight[:-1].to(device)  # exclude padding_idx\n",
    "    with torch.no_grad():\n",
    "        target_emb = weight[target_idx]\n",
    "        sims = torch.matmul(weight, target_emb)\n",
    "    sims[target_idx] = -np.inf  # exclude self\n",
    "    topk = torch.topk(sims, k=top_k)\n",
    "    return [\n",
    "        (id_mapper.index_to_item[int(i)], float(s))\n",
    "        for i, s in zip(topk.indices, topk.values)\n",
    "    ]\n",
    "\n",
    "# === Compute Recommendations ===\n",
    "def compute_recommendations():\n",
    "    model, id_mapper = load_model()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    all_indices = list(id_mapper.index_to_item.keys())\n",
    "\n",
    "    # Load all embeddings from Qdrant\n",
    "    records = qdrant_client.retrieve(CONFIG[\"qdrant_collection_name\"], ids=all_indices, with_vectors=True)\n",
    "    id_to_vec = {rec.id: rec.vector for rec in records if rec.vector is not None}\n",
    "\n",
    "    recs = []\n",
    "\n",
    "    for i in tqdm(range(0, len(all_indices), CONFIG[\"batch_size\"]), desc=\"Processing\"):\n",
    "        batch_indices = all_indices[i:i + CONFIG[\"batch_size\"]]\n",
    "        batch_vectors = [id_to_vec.get(idx) for idx in batch_indices]\n",
    "\n",
    "        batch_neighbors = []\n",
    "        for idx, vec in zip(batch_indices, batch_vectors):\n",
    "            if vec is None:\n",
    "                batch_neighbors.append([])\n",
    "                continue\n",
    "            neighbors = qdrant_client.search(\n",
    "                collection_name=CONFIG[\"qdrant_collection_name\"],\n",
    "                query_vector=vec,\n",
    "                limit=CONFIG[\"top_K\"] + 1,\n",
    "            )\n",
    "            # Remove self from results\n",
    "            neighbor_ids = [n.id for n in neighbors if n.id != idx][:CONFIG[\"top_K\"]]\n",
    "            batch_neighbors.append(neighbor_ids)\n",
    "\n",
    "        batch_scores = []\n",
    "        for idx, neighbors in zip(batch_indices, batch_neighbors):\n",
    "            if not neighbors:\n",
    "                batch_scores.append([])\n",
    "                continue\n",
    "\n",
    "            target_id = id_mapper.index_to_item[idx]\n",
    "            neighbor_ids = [id_mapper.index_to_item[nid] for nid in neighbors]\n",
    "\n",
    "            # Compute scores using model\n",
    "            sample_input = {\n",
    "                \"target_items\": [target_id] * len(neighbor_ids),\n",
    "                \"context_items\": neighbor_ids\n",
    "            }\n",
    "            try:\n",
    "                scores = predict_batch(model, id_mapper, sample_input, device)\n",
    "                batch_scores.append(scores.tolist())\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Prediction failed for {target_id}: {e}\")\n",
    "                batch_scores.append([0.0] * len(neighbor_ids))\n",
    "\n",
    "        # Write to Redis and JSONL\n",
    "        for idx, neighbors, scores in zip(batch_indices, batch_neighbors, batch_scores):\n",
    "            if not neighbors:\n",
    "                continue\n",
    "            sorted_pairs = sorted(zip(neighbors, scores), key=lambda x: x[1], reverse=True)\n",
    "            top_neighbors, top_scores = zip(*sorted_pairs[:CONFIG[\"top_k\"]])\n",
    "            neighbor_ids = [id_mapper.index_to_item[n] for n in top_neighbors]\n",
    "            target_id = id_mapper.index_to_item[idx]\n",
    "\n",
    "            rec = {\n",
    "                \"target_item\": target_id,\n",
    "                \"rec_item_ids\": neighbor_ids,\n",
    "                \"rec_scores\": list(top_scores)\n",
    "            }\n",
    "\n",
    "            recs.append(rec)\n",
    "            redis_client.set(f\"rec:{target_id}\", json.dumps(rec))\n",
    "            \n",
    "    os.makedirs(os.path.dirname(CONFIG[\"output_file\"]), exist_ok=True)\n",
    "    with open(CONFIG[\"output_file\"], \"w\") as f:\n",
    "        for rec in recs:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Completed. Stored {len(recs)} recommendations to Redis and {CONFIG['output_file']}\")\n",
    "\n",
    "# === Inference Utility for Batch Prediction ===\n",
    "def predict_batch(model, id_mapper, batch: dict, device=torch.device(\"cpu\")) -> np.ndarray:\n",
    "    tgt = torch.tensor([id_mapper.item_to_index[i] for i in batch[\"target_items\"]], device=device)\n",
    "    ctx = torch.tensor([id_mapper.item_to_index[i] for i in batch[\"context_items\"]], device=device)\n",
    "    with torch.no_grad():\n",
    "        return model(tgt, ctx).cpu().numpy()\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    compute_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b2d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recommendation found:\n",
      "{\n",
      "  \"target_item\": \"B0002YV94U\",\n",
      "  \"rec_item_ids\": [\n",
      "    \"B087GTFML5\",\n",
      "    \"B087GY3RJS\",\n",
      "    \"B08KXGHX9V\",\n",
      "    \"B00K8A08YU\",\n",
      "    \"B0C3GL76CN\",\n",
      "    \"B00DW1JT3I\",\n",
      "    \"B00K5OLKCI\",\n",
      "    \"B00U5MVOX0\",\n",
      "    \"B0CH269DQ5\",\n",
      "    \"B00LDX3OFQ\"\n",
      "  ],\n",
      "  \"rec_scores\": [\n",
      "    0.6255476474761963,\n",
      "    0.6213389039039612,\n",
      "    0.616352379322052,\n",
      "    0.6159809231758118,\n",
      "    0.6147825121879578,\n",
      "    0.6140996813774109,\n",
      "    0.6071546673774719,\n",
      "    0.603735089302063,\n",
      "    0.6013337969779968,\n",
      "    0.6004250645637512\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "# Cấu hình Redis (đảm bảo giống CONFIG trong script gốc)\n",
    "redis_client = redis.Redis(\n",
    "    host=\"localhost\",        # Hoặc thay bằng giá trị từ CONFIG[\"redis_host\"]\n",
    "    port=6379,               # CONFIG[\"redis_port\"]\n",
    "    db=0,                    # CONFIG[\"redis_db\"]\n",
    "    decode_responses=True,   # Đảm bảo trả về string thay vì bytes\n",
    "    password=\"123456\"    # Nếu có auth\n",
    ")\n",
    "\n",
    "# Item ID cần truy vấn\n",
    "item_id = \"B0002YV94U\"\n",
    "key = f\"rec:{item_id}\"\n",
    "\n",
    "# Lấy dữ liệu từ Redis\n",
    "rec_json = redis_client.get(key)\n",
    "\n",
    "if rec_json:\n",
    "    rec_data = json.loads(rec_json)\n",
    "    print(\"✅ Recommendation found:\")\n",
    "    print(json.dumps(rec_data, indent=2))\n",
    "else:\n",
    "    print(f\"❌ No recommendation found for item {item_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db2bc6",
   "metadata": {},
   "source": [
    "## Load popular item to Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f932e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Top popular parent_asin (score-based):\n",
      "     parent_asin  rating_count  rating_avg  score\n",
      "3934  B0BW3QTWJJ           438    4.780822  780.0\n",
      "2559  B07C4NGT17           226    4.867257  422.0\n",
      "801   B0054TRQA4           216    4.750000  378.0\n",
      "1223  B00D8STBHY           202    4.816832  367.0\n",
      "3589  B09QPXVW35           166    4.885542  313.0\n",
      "...          ...           ...         ...    ...\n",
      "91    B000067NXE            45    4.333333   60.0\n",
      "3290  B08PMPDGXM            36    4.638889   59.0\n",
      "311   B000NV7L0I            42    4.404762   59.0\n",
      "103   B000088UPW            56    4.053571   59.0\n",
      "1451  B00IL7IFP6            33    4.787879   59.0\n",
      "\n",
      "[500 rows x 4 columns]\n",
      "\n",
      "✅ Đã lưu 500 popular parent_asin vào Redis key: popular_parent_asin_score\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import redis\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = os.path.abspath(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "POSTGRES_URI = os.environ[\"POSTGRES_URI_OLTP\"]\n",
    "conn_str = POSTGRES_URI\n",
    "table = \"public.reviews\"\n",
    "\n",
    "# Redis config\n",
    "redis_client = redis.Redis(\n",
    "    host=\"localhost\",\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=True,\n",
    "    password=\"123456\"\n",
    ")\n",
    "\n",
    "# 1. Load từ PostgreSQL\n",
    "engine = create_engine(conn_str)\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        parent_asin,\n",
    "        rating\n",
    "    FROM {table}\n",
    "    WHERE parent_asin IS NOT NULL\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# 2. Tính toán thống kê\n",
    "agg_df = (\n",
    "    df.groupby(\"parent_asin\")\n",
    "    .agg(rating_count=(\"rating\", \"count\"), rating_avg=(\"rating\", \"mean\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 3. Tính popularity score\n",
    "agg_df[\"score\"] = agg_df[\"rating_count\"] * (agg_df[\"rating_avg\"] - 3.0)\n",
    "\n",
    "# 4. Lấy top phổ biến theo score\n",
    "TOP_K = 500\n",
    "top_df = agg_df.sort_values(\"score\", ascending=False).head(TOP_K)\n",
    "\n",
    "# ✅ In ra để kiểm tra trước khi lưu\n",
    "print(\"🔍 Top popular parent_asin (score-based):\")\n",
    "print(top_df[[\"parent_asin\", \"rating_count\", \"rating_avg\", \"score\"]])\n",
    "\n",
    "# 5. Ghi vào Redis nếu OK\n",
    "redis_key = \"popular_parent_asin_score\"\n",
    "redis_client.delete(redis_key)\n",
    "\n",
    "for _, row in top_df.iterrows():\n",
    "    redis_client.zadd(redis_key, {row[\"parent_asin\"]: float(row[\"score\"])})\n",
    "\n",
    "print(f\"\\n✅ Đã lưu {len(top_df)} popular parent_asin vào Redis key: {redis_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b0a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Top 10 popular parent_asin from Redis:\n",
      " 1. ASIN: B0BW3QTWJJ | Score: 780.00\n",
      " 2. ASIN: B07C4NGT17 | Score: 422.00\n",
      " 3. ASIN: B0054TRQA4 | Score: 378.00\n",
      " 4. ASIN: B00D8STBHY | Score: 367.00\n",
      " 5. ASIN: B09QPXVW35 | Score: 313.00\n",
      " 6. ASIN: B07N29HQMN | Score: 257.00\n",
      " 7. ASIN: B00FZMDAO6 | Score: 248.00\n",
      " 8. ASIN: B0BG94QRLZ | Score: 241.00\n",
      " 9. ASIN: B09PH8LV57 | Score: 238.00\n",
      "10. ASIN: B004S8F7QM | Score: 238.00\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "\n",
    "# Kết nối Redis\n",
    "redis_client = redis.Redis(\n",
    "    host=\"localhost\",\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=True,\n",
    "    password=\"123456\"\n",
    ")\n",
    "\n",
    "# Redis key bạn đã lưu trước đó\n",
    "redis_key = \"popular_parent_asin_score\"\n",
    "\n",
    "# Truy vấn top 10 phổ biến nhất (score cao nhất)\n",
    "top_items = redis_client.zrevrange(redis_key, 0, 9, withscores=True)\n",
    "\n",
    "# In ra kết quả\n",
    "print(\"🔥 Top 10 popular parent_asin from Redis:\")\n",
    "for rank, (asin, score) in enumerate(top_items, start=1):\n",
    "    print(f\"{rank:2d}. ASIN: {asin} | Score: {score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datn-recsys)",
   "language": "python",
   "name": "datn-recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
