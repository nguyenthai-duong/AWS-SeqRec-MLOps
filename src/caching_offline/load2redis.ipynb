{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b912f8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2d723e1f8743b9a4d04569eaede57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/29 08:56:40 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.7.1+cu126'\n",
      "/home/duong/Documents/datn1/.venv/lib/python3.11/site-packages/torch/serialization.py:1488: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4da58d4dd44a90aa26226902284c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model v1 (run_id=aac89aa42a3c447eac1294546df933e6) and IDMapper.\n",
      "\n",
      "üîπ Embedding for 'B00000IV95' (index 36): [ 0.05149313  0.0443965  -0.01442701  0.07874709  0.07239346]\n",
      "\n",
      "üîπ Top 5 similar items to 'B00000IV95':\n",
      " - B0C3H818H4 (index 4009): score = 2.2501\n",
      " - B0C48KPLZ2 (index 4026): score = 1.6529\n",
      " - 0975277324 (index 5): score = 1.6294\n",
      " - B0C1FX3BGK (index 3976): score = 1.5719\n",
      " - B00000IV35 (index 35): score = 1.5157\n",
      "\n",
      "üîπ Batch prediction scores:\n",
      " - (B00000IV95, B0792X1RSC): score = 0.5309\n",
      " - (B0792X1RSC, B00000IV95): score = 0.5309\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "\n",
    "# === C·∫•u h√¨nh MLflow + MinIO ===\n",
    "def configure_mlflow(s3_endpoint: str, aws_key: str, aws_secret: str, tracking_uri: str):\n",
    "    os.environ.update({\n",
    "        \"AWS_ACCESS_KEY_ID\": aws_key,\n",
    "        \"AWS_SECRET_ACCESS_KEY\": aws_secret,\n",
    "        \"MLFLOW_S3_ENDPOINT_URL\": s3_endpoint,\n",
    "        \"MLFLOW_S3_IGNORE_TLS\": \"true\"\n",
    "    })\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "    session = boto3.session.Session(\n",
    "        aws_access_key_id=aws_key,\n",
    "        aws_secret_access_key=aws_secret\n",
    "    )\n",
    "    S3ArtifactRepository._get_s3_client = lambda self: session.client(\"s3\", endpoint_url=s3_endpoint)\n",
    "\n",
    "# === IDMapper ƒë∆°n gi·∫£n ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index: Dict[str, int] = mapping[\"item_to_index\"]\n",
    "        self.index_to_item: Dict[int, str] = mapping[\"index_to_item\"]\n",
    "\n",
    "# === Load TorchScript model + IDMapper t·ª´ registered model ===\n",
    "def load_champion_model(model_name: str) -> Tuple[torch.jit.ScriptModule, SimpleIDMapper]:\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    champs = [v for v in versions if v.tags.get(\"champion\", \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise RuntimeError(f\"‚ùå No champion version found for model '{model_name}'\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model = mlflow.pytorch.load_model(f\"models:/{model_name}/{champ.version}\")\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    print(f\"‚úÖ Loaded model v{champ.version} (run_id={run_id}) and IDMapper.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "# === Inference Utilities ===\n",
    "def get_item_embedding(model, id_mapper, item_id, device=torch.device(\"cpu\")) -> torch.Tensor:\n",
    "    idx = torch.tensor([id_mapper.item_to_index[item_id]], device=device)\n",
    "    return model.embeddings(idx).squeeze(0)\n",
    "\n",
    "def get_topk_similar(model, id_mapper, item_id, top_k=10, device=torch.device(\"cpu\")) -> List[Tuple[str, int, float]]:\n",
    "    target_idx = id_mapper.item_to_index[item_id]\n",
    "    weight = model.embeddings.weight[:-1].to(device)  # exclude padding_idx\n",
    "    with torch.no_grad():\n",
    "        target_emb = weight[target_idx]\n",
    "        sims = torch.matmul(weight, target_emb)\n",
    "    sims[target_idx] = -np.inf  # exclude self\n",
    "    topk = torch.topk(sims, k=top_k)\n",
    "    return [\n",
    "        (id_mapper.index_to_item[int(i)], int(i), float(s))\n",
    "        for i, s in zip(topk.indices, topk.values)\n",
    "    ]\n",
    "\n",
    "def predict_batch(model, id_mapper, batch: Dict[str, List[str]], device=torch.device(\"cpu\")) -> np.ndarray:\n",
    "    tgt = torch.tensor([id_mapper.item_to_index[i] for i in batch[\"target_items\"]], device=device)\n",
    "    ctx = torch.tensor([id_mapper.item_to_index[i] for i in batch[\"context_items\"]], device=device)\n",
    "    with torch.no_grad():\n",
    "        return model(tgt, ctx).cpu().numpy()\n",
    "\n",
    "# === Ch·∫°y th·ª≠ ===\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. C·∫•u h√¨nh\n",
    "    configure_mlflow(\n",
    "        s3_endpoint=\"http://127.0.0.1:9010\",\n",
    "        aws_key=\"admin\",\n",
    "        aws_secret=\"Password1234\",\n",
    "        tracking_uri=\"http://localhost:5002\"\n",
    "    )\n",
    "\n",
    "    # 2. Load model + IDMapper\n",
    "    model_name = \"item2vec_skipgram\"\n",
    "    model, id_mapper = load_champion_model(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 3. In embedding\n",
    "    test_item = \"B00000IV95\"\n",
    "    test_idx = id_mapper.item_to_index[test_item]\n",
    "    emb = get_item_embedding(model, id_mapper, test_item, device)\n",
    "    print(f\"\\nüîπ Embedding for '{test_item}' (index {test_idx}): {emb[:5].detach().cpu().numpy()}\")\n",
    "\n",
    "    # 4. T√¨m top K t∆∞∆°ng t·ª±\n",
    "    topk = get_topk_similar(model, id_mapper, test_item, top_k=5, device=device)\n",
    "    print(f\"\\nüîπ Top 5 similar items to '{test_item}':\")\n",
    "    for iid, idx, score in topk:\n",
    "        print(f\" - {iid} (index {idx}): score = {score:.4f}\")\n",
    "\n",
    "    # 5. D·ª± ƒëo√°n batch\n",
    "    batch = {\n",
    "        \"target_items\": [test_item, \"B0792X1RSC\"],\n",
    "        \"context_items\": [\"B0792X1RSC\", test_item]\n",
    "    }\n",
    "    scores = predict_batch(model, id_mapper, batch, device)\n",
    "    print(f\"\\nüîπ Batch prediction scores:\")\n",
    "    for t, c, s in zip(batch[\"target_items\"], batch[\"context_items\"], scores):\n",
    "        print(f\" - ({t}, {c}): score = {s:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941bd42",
   "metadata": {},
   "source": [
    "## Load embedding to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b76cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting embedding indexing...\n",
      "üì¶ Loading model from URI: models:/item2vec_skipgram/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41322d49b61f4234bd606033e492a1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/29 09:00:35 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.7.1+cu126'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226f79464b7e446bb95adc1ffc83bd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model version 1 (run_id=aac89aa42a3c447eac1294546df933e6) with 4143 items.\n",
      "‚úÖ Extracted embeddings with shape (4143, 256)\n",
      "üåê Connecting to Qdrant at http://localhost:6333\n",
      "‚úÖ Created new collection 'item2vec_collection' with dim=256\n",
      "‚úÖ Upserted 4143 embeddings into Qdrant collection 'item2vec_collection'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "import boto3\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "PROJECT_ROOT = \"/home/duong/Documents/datn1/src/model_item2vec\"\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MLflow config\n",
    "S3_ENDPOINT = \"http://127.0.0.1:9010\"\n",
    "AWS_KEY = \"admin\"\n",
    "AWS_SECRET = \"Password1234\"\n",
    "TRACKING_URI = \"http://localhost:5002\"\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_KEY\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = S3_ENDPOINT\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "# Configure boto3 for MinIO\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=AWS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET\n",
    ")\n",
    "S3ArtifactRepository._get_s3_client = lambda self: session.client(\"s3\", endpoint_url=S3_ENDPOINT)\n",
    "\n",
    "# Qdrant config\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "QDRANT_COLLECTION = os.getenv(\"QDRANT_COLLECTION_NAME\", \"item2vec_collection\")\n",
    "\n",
    "MODEL_NAME = \"item2vec_skipgram\"\n",
    "TAG_NAME = \"champion\"\n",
    "\n",
    "# === SimpleIDMapper ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index = mapping[\"item_to_index\"]\n",
    "        self.index_to_item = mapping[\"index_to_item\"]\n",
    "\n",
    "# === STEP 1: LOAD MODEL AND ID MAPPING ===\n",
    "def load_model_from_mlflow(model_name: str, tag: str = \"champion\") -> tuple:\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    champs = [v for v in versions if v.tags.get(tag, \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise ValueError(f\"‚ùå No version tagged '{tag}' found for model '{model_name}'.\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model_uri = f\"models:/{model_name}/{champ.version}\"\n",
    "    print(f\"üì¶ Loading model from URI: {model_uri}\")\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    print(f\"‚úÖ Loaded model version {champ.version} (run_id={run_id}) with {len(id_mapper.item_to_index)} items.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "# === STEP 2: EXTRACT EMBEDDINGS ===\n",
    "def get_all_embeddings(model, id_mapper: SimpleIDMapper) -> tuple:\n",
    "    item_ids = list(id_mapper.item_to_index.keys())\n",
    "    item_indices = [id_mapper.item_to_index[id_] for id_ in item_ids]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    tensor_indices = torch.tensor(item_indices, device=device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.embeddings(tensor_indices).detach().cpu().numpy()\n",
    "    print(f\"‚úÖ Extracted embeddings with shape {embeddings.shape}\")\n",
    "    return item_ids, embeddings\n",
    "\n",
    "# === STEP 3: INDEX TO QDRANT (delete and recreate collection) ===\n",
    "def index_embeddings_to_qdrant(item_ids: list, embeddings: np.ndarray, qdrant_url: str, collection_name: str):\n",
    "    try:\n",
    "        client = QdrantClient(url=qdrant_url)\n",
    "        print(f\"üåê Connecting to Qdrant at {qdrant_url}\")\n",
    "\n",
    "        vector_dim = embeddings.shape[1]\n",
    "\n",
    "        # Delete collection if it exists\n",
    "        if client.collection_exists(collection_name):\n",
    "            client.delete_collection(collection_name)\n",
    "            print(f\"üóëÔ∏è Deleted existing collection '{collection_name}'.\")\n",
    "\n",
    "        # Create new collection\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_dim, distance=Distance.COSINE),\n",
    "        )\n",
    "        print(f\"‚úÖ Created new collection '{collection_name}' with dim={vector_dim}\")\n",
    "\n",
    "        # Prepare points using sequential IDs\n",
    "        points = [\n",
    "            PointStruct(id=idx, vector=vec.tolist(), payload={\"item_id\": item_id})\n",
    "            for idx, (item_id, vec) in enumerate(zip(item_ids, embeddings))\n",
    "        ]\n",
    "\n",
    "        # Upsert points\n",
    "        upsert_result = client.upsert(collection_name=collection_name, points=points)\n",
    "        assert str(upsert_result.status) == \"completed\"\n",
    "        print(f\"‚úÖ Upserted {len(points)} embeddings into Qdrant collection '{collection_name}'\")\n",
    "\n",
    "    except UnexpectedResponse as e:\n",
    "        print(\"‚ùå Qdrant returned unexpected response (possibly 404 or 400).\")\n",
    "        print(\"‚û°Ô∏è Check that the endpoint is correct and Qdrant is running.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Failed to index embeddings to Qdrant.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    print(\"üöÄ Starting embedding indexing...\")\n",
    "    model, id_mapper = load_model_from_mlflow(MODEL_NAME, TAG_NAME)\n",
    "    item_ids, embeddings = get_all_embeddings(model, id_mapper)\n",
    "    index_embeddings_to_qdrant(item_ids, embeddings, QDRANT_URL, QDRANT_COLLECTION)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489b078",
   "metadata": {},
   "source": [
    "### Test Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5fd1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting similarity search...\n",
      "üì¶ Loading model from URI: models:/item2vec_skipgram/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ec8f1e68b3439e9cdbc5e4fe4b345d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/29 09:00:46 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.7.1+cu126'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c922ce11124e6093d88afb7615a464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model version 1 (run_id=aac89aa42a3c447eac1294546df933e6) with 4143 items.\n",
      "‚úÖ Generated embedding for 'B00EMGM1JQ' with shape (256,)\n",
      "üåê Connecting to Qdrant at http://localhost:6333\n",
      "\n",
      "üîç Top 5 items similar to 'B00EMGM1JQ':\n",
      "üü¢ B00EMGM1JQ      | Score: 1.0000\n",
      "üü¢ B00C6Q4HEQ      | Score: 0.7464\n",
      "üü¢ B00C6Q5S44      | Score: 0.6372\n",
      "üü¢ B00C6Q1Z6E      | Score: 0.6326\n",
      "üü¢ B00CQHZ2LW      | Score: 0.6079\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import mlflow\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "import boto3\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "QDRANT_COLLECTION = \"item2vec_collection\"\n",
    "MODEL_NAME = \"item2vec_skipgram\"\n",
    "TAG_NAME = \"champion\"\n",
    "ITEM_ID = \"B00EMGM1JQ\"\n",
    "TOP_K = 5\n",
    "\n",
    "S3_ENDPOINT = \"http://127.0.0.1:9010\"\n",
    "AWS_KEY = \"admin\"\n",
    "AWS_SECRET = \"Password1234\"\n",
    "TRACKING_URI = \"http://localhost:5002\"\n",
    "\n",
    "# Configure MLflow and MinIO\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_KEY\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = S3_ENDPOINT\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=AWS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET\n",
    ")\n",
    "S3ArtifactRepository._get_s3_client = lambda self: session.client(\"s3\", endpoint_url=S3_ENDPOINT)\n",
    "\n",
    "# === SimpleIDMapper ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index = mapping[\"item_to_index\"]\n",
    "        self.index_to_item = mapping[\"index_to_item\"]\n",
    "\n",
    "# === STEP 1: Load model and ID mapping ===\n",
    "def load_model_and_mapping(model_name: str, tag: str = \"champion\") -> tuple:\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    champs = [v for v in versions if v.tags.get(tag, \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise ValueError(f\"‚ùå No version tagged '{tag}' found for model '{model_name}'.\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model_uri = f\"models:/{model_name}/{champ.version}\"\n",
    "    print(f\"üì¶ Loading model from URI: {model_uri}\")\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    print(f\"‚úÖ Loaded model version {champ.version} (run_id={run_id}) with {len(id_mapper.item_to_index)} items.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "# === STEP 2: Get embedding vector for ITEM_ID ===\n",
    "def get_item_embedding(model, id_mapper: SimpleIDMapper, item_id: str, device: torch.device) -> np.ndarray:\n",
    "    idx = id_mapper.item_to_index.get(item_id)\n",
    "    if idx is None:\n",
    "        raise ValueError(f\"‚ùå Item ID '{item_id}' not found in ID mapping\")\n",
    "    tensor_idx = torch.tensor([idx], device=device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.embeddings(tensor_idx).detach().cpu().numpy()[0]\n",
    "    return embedding\n",
    "\n",
    "# === STEP 3: Query Qdrant with embedding vector ===\n",
    "def search_similar_items(qdrant_url: str, collection_name: str, embedding: np.ndarray, top_k: int) -> list:\n",
    "    try:\n",
    "        qdrant = QdrantClient(url=qdrant_url)\n",
    "        print(f\"üåê Connecting to Qdrant at {qdrant_url}\")\n",
    "        if not qdrant.collection_exists(collection_name):\n",
    "            raise ValueError(f\"‚ùå Collection '{collection_name}' does not exist in Qdrant\")\n",
    "        \n",
    "        results = qdrant.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=embedding.tolist(),\n",
    "            limit=top_k,\n",
    "        )\n",
    "        return results\n",
    "    except UnexpectedResponse as e:\n",
    "        print(\"‚ùå Qdrant returned unexpected response (possibly 404 or 400).\")\n",
    "        print(\"‚û°Ô∏è Check that the endpoint is correct and Qdrant is running.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Failed to query Qdrant.\")\n",
    "        print(str(e))\n",
    "        raise\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    print(\"üöÄ Starting similarity search...\")\n",
    "    # Load model and ID mapping\n",
    "    model, id_mapper = load_model_and_mapping(MODEL_NAME, TAG_NAME)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get embedding for ITEM_ID\n",
    "    embedding = get_item_embedding(model, id_mapper, ITEM_ID, device)\n",
    "    print(f\"‚úÖ Generated embedding for '{ITEM_ID}' with shape {embedding.shape}\")\n",
    "    \n",
    "    # Query Qdrant\n",
    "    results = search_similar_items(QDRANT_URL, QDRANT_COLLECTION, embedding, TOP_K)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüîç Top {TOP_K} items similar to '{ITEM_ID}':\")\n",
    "    for r in results:\n",
    "        item = r.payload.get(\"item_id\", \"<missing>\")\n",
    "        print(f\"üü¢ {item:15} | Score: {r.score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f64b16",
   "metadata": {},
   "source": [
    "## Load pre-recommend to Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e711ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80a7749a6584990ae8c454ab54b68d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/29 09:04:04 WARNING mlflow.pytorch: Stored model version '2.6.0+cu124' does not match installed PyTorch version '2.7.1+cu126'\n",
      "/home/duong/Documents/datn1/.venv/lib/python3.11/site-packages/torch/serialization.py:1488: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f77416c2ed04c9eb7f97e4c4d4a7907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model v1 (run_id=aac89aa42a3c447eac1294546df933e6) and IDMapper.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd8c194e8de49068f3ca41fb562ef96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed. Stored 4143 recommendations to Redis and ../../data/batch_recs.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import redis\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import sys\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm.auto import tqdm\n",
    "import boto3\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.s3_artifact_repo import S3ArtifactRepository\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "PROJECT_ROOT = \"/home/duong/Documents/datn1/src/model_item2vec\"\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "CONFIG = {\n",
    "    \"mlf_model_name\": \"item2vec_skipgram\",\n",
    "    \"qdrant_url\": f\"{os.getenv('QDRANT_HOST', 'localhost')}:{os.getenv('QDRANT_PORT', '6333')}\",\n",
    "    \"qdrant_collection_name\": os.getenv(\"QDRANT_COLLECTION_NAME\", \"item2vec_collection\"),\n",
    "    \"redis_host\": os.getenv(\"REDIS_HOST\", \"localhost\"),\n",
    "    \"redis_port\": 6379,\n",
    "    \"redis_db\": int(os.getenv(\"REDIS_DB\", 0)),\n",
    "    \"batch_size\": 256,\n",
    "    \"top_k\": 10,\n",
    "    \"top_K\": 100,\n",
    "    \"output_file\": \"../../data/batch_recs.jsonl\",\n",
    "    \"s3_endpoint\": \"http://127.0.0.1:9010\",\n",
    "    \"aws_key\": \"admin\",\n",
    "    \"aws_secret\": \"Password1234\"\n",
    "}\n",
    "\n",
    "# === ENV SETUP ===\n",
    "def configure_mlflow():\n",
    "    os.environ.update({\n",
    "        \"AWS_ACCESS_KEY_ID\": CONFIG[\"aws_key\"],\n",
    "        \"AWS_SECRET_ACCESS_KEY\": CONFIG[\"aws_secret\"],\n",
    "        \"MLFLOW_S3_ENDPOINT_URL\": CONFIG[\"s3_endpoint\"],\n",
    "        \"MLFLOW_S3_IGNORE_TLS\": \"true\"\n",
    "    })\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5002\")\n",
    "\n",
    "    session = boto3.session.Session(\n",
    "        aws_access_key_id=CONFIG[\"aws_key\"],\n",
    "        aws_secret_access_key=CONFIG[\"aws_secret\"]\n",
    "    )\n",
    "    S3ArtifactRepository._get_s3_client = lambda self: session.client(\"s3\", endpoint_url=CONFIG[\"s3_endpoint\"])\n",
    "\n",
    "# === CLIENTS ===\n",
    "redis_client = redis.Redis(\n",
    "    host=CONFIG[\"redis_host\"],\n",
    "    port=CONFIG[\"redis_port\"],\n",
    "    db=CONFIG[\"redis_db\"],\n",
    "    decode_responses=True,\n",
    "    password=\"123456\"\n",
    ")\n",
    "qdrant_client = QdrantClient(url=CONFIG[\"qdrant_url\"])\n",
    "\n",
    "# === SimpleIDMapper ===\n",
    "class SimpleIDMapper:\n",
    "    def __init__(self, mapping_path: str):\n",
    "        with open(mapping_path, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.item_to_index = mapping[\"item_to_index\"]\n",
    "        # Convert list to dict: index (int) -> item_id (str)\n",
    "        self.index_to_item = {i: item_id for i, item_id in enumerate(mapping[\"index_to_item\"])}\n",
    "# === Load TorchScript model + IDMapper ===\n",
    "def load_model():\n",
    "    configure_mlflow()\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{CONFIG['mlf_model_name']}'\")\n",
    "    champs = [v for v in versions if v.tags.get(\"champion\", \"\").lower() == \"true\"]\n",
    "    if not champs:\n",
    "        raise RuntimeError(f\"‚ùå No champion version found for model '{CONFIG['mlf_model_name']}'\")\n",
    "\n",
    "    champ = max(champs, key=lambda v: v.creation_timestamp)\n",
    "    run_id = champ.run_id\n",
    "    model = mlflow.pytorch.load_model(f\"models:/{CONFIG['mlf_model_name']}/{champ.version}\")\n",
    "    model.eval()\n",
    "\n",
    "    id_mapper_path = f\"runs:/{run_id}/id_mapper/id_mapper.json\"\n",
    "    id_mapper_local = mlflow.artifacts.download_artifacts(id_mapper_path)\n",
    "    id_mapper = SimpleIDMapper(id_mapper_local)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"‚úÖ Loaded model v{champ.version} (run_id={run_id}) and IDMapper.\")\n",
    "    return model, id_mapper\n",
    "\n",
    "# === Inference Utilities ===\n",
    "def get_item_embedding(model, id_mapper, item_id, device=torch.device(\"cpu\")) -> torch.Tensor:\n",
    "    idx = torch.tensor([id_mapper.item_to_index[item_id]], device=device)\n",
    "    return model.embeddings(idx).squeeze(0)\n",
    "\n",
    "def get_topk_similar(model, id_mapper, item_id, top_k=10, device=torch.device(\"cpu\")) -> list:\n",
    "    target_idx = id_mapper.item_to_index[item_id]\n",
    "    weight = model.embeddings.weight[:-1].to(device)  # exclude padding_idx\n",
    "    with torch.no_grad():\n",
    "        target_emb = weight[target_idx]\n",
    "        sims = torch.matmul(weight, target_emb)\n",
    "    sims[target_idx] = -np.inf  # exclude self\n",
    "    topk = torch.topk(sims, k=top_k)\n",
    "    return [\n",
    "        (id_mapper.index_to_item[int(i)], float(s))\n",
    "        for i, s in zip(topk.indices, topk.values)\n",
    "    ]\n",
    "\n",
    "# === Compute Recommendations ===\n",
    "def compute_recommendations():\n",
    "    model, id_mapper = load_model()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    all_indices = list(id_mapper.index_to_item.keys())\n",
    "\n",
    "    # Load all embeddings from Qdrant\n",
    "    records = qdrant_client.retrieve(CONFIG[\"qdrant_collection_name\"], ids=all_indices, with_vectors=True)\n",
    "    id_to_vec = {rec.id: rec.vector for rec in records if rec.vector is not None}\n",
    "\n",
    "    recs = []\n",
    "\n",
    "    for i in tqdm(range(0, len(all_indices), CONFIG[\"batch_size\"]), desc=\"Processing\"):\n",
    "        batch_indices = all_indices[i:i + CONFIG[\"batch_size\"]]\n",
    "        batch_vectors = [id_to_vec.get(idx) for idx in batch_indices]\n",
    "\n",
    "        batch_neighbors = []\n",
    "        for idx, vec in zip(batch_indices, batch_vectors):\n",
    "            if vec is None:\n",
    "                batch_neighbors.append([])\n",
    "                continue\n",
    "            neighbors = qdrant_client.search(\n",
    "                collection_name=CONFIG[\"qdrant_collection_name\"],\n",
    "                query_vector=vec,\n",
    "                limit=CONFIG[\"top_K\"] + 1,\n",
    "            )\n",
    "            # Remove self from results\n",
    "            neighbor_ids = [n.id for n in neighbors if n.id != idx][:CONFIG[\"top_K\"]]\n",
    "            batch_neighbors.append(neighbor_ids)\n",
    "\n",
    "        batch_scores = []\n",
    "        for idx, neighbors in zip(batch_indices, batch_neighbors):\n",
    "            if not neighbors:\n",
    "                batch_scores.append([])\n",
    "                continue\n",
    "\n",
    "            target_id = id_mapper.index_to_item[idx]\n",
    "            neighbor_ids = [id_mapper.index_to_item[nid] for nid in neighbors]\n",
    "\n",
    "            # Compute scores using model\n",
    "            sample_input = {\n",
    "                \"target_items\": [target_id] * len(neighbor_ids),\n",
    "                \"context_items\": neighbor_ids\n",
    "            }\n",
    "            try:\n",
    "                scores = predict_batch(model, id_mapper, sample_input, device)\n",
    "                batch_scores.append(scores.tolist())\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Prediction failed for {target_id}: {e}\")\n",
    "                batch_scores.append([0.0] * len(neighbor_ids))\n",
    "\n",
    "        # Write to Redis and JSONL\n",
    "        for idx, neighbors, scores in zip(batch_indices, batch_neighbors, batch_scores):\n",
    "            if not neighbors:\n",
    "                continue\n",
    "            sorted_pairs = sorted(zip(neighbors, scores), key=lambda x: x[1], reverse=True)\n",
    "            top_neighbors, top_scores = zip(*sorted_pairs[:CONFIG[\"top_k\"]])\n",
    "            neighbor_ids = [id_mapper.index_to_item[n] for n in top_neighbors]\n",
    "            target_id = id_mapper.index_to_item[idx]\n",
    "\n",
    "            rec = {\n",
    "                \"target_item\": target_id,\n",
    "                \"rec_item_ids\": neighbor_ids,\n",
    "                \"rec_scores\": list(top_scores)\n",
    "            }\n",
    "\n",
    "            recs.append(rec)\n",
    "            redis_client.set(f\"rec:{target_id}\", json.dumps(rec))\n",
    "            \n",
    "    os.makedirs(os.path.dirname(CONFIG[\"output_file\"]), exist_ok=True)\n",
    "    with open(CONFIG[\"output_file\"], \"w\") as f:\n",
    "        for rec in recs:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Completed. Stored {len(recs)} recommendations to Redis and {CONFIG['output_file']}\")\n",
    "\n",
    "# === Inference Utility for Batch Prediction ===\n",
    "def predict_batch(model, id_mapper, batch: dict, device=torch.device(\"cpu\")) -> np.ndarray:\n",
    "    tgt = torch.tensor([id_mapper.item_to_index[i] for i in batch[\"target_items\"]], device=device)\n",
    "    ctx = torch.tensor([id_mapper.item_to_index[i] for i in batch[\"context_items\"]], device=device)\n",
    "    with torch.no_grad():\n",
    "        return model(tgt, ctx).cpu().numpy()\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    compute_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b2d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Recommendation found:\n",
      "{\n",
      "  \"target_item\": \"B0002YV94U\",\n",
      "  \"rec_item_ids\": [\n",
      "    \"B087GTFML5\",\n",
      "    \"B087GY3RJS\",\n",
      "    \"B08KXGHX9V\",\n",
      "    \"B00K8A08YU\",\n",
      "    \"B0C3GL76CN\",\n",
      "    \"B00DW1JT3I\",\n",
      "    \"B00K5OLKCI\",\n",
      "    \"B00U5MVOX0\",\n",
      "    \"B0CH269DQ5\",\n",
      "    \"B00LDX3OFQ\"\n",
      "  ],\n",
      "  \"rec_scores\": [\n",
      "    0.6255476474761963,\n",
      "    0.6213389039039612,\n",
      "    0.616352379322052,\n",
      "    0.6159809231758118,\n",
      "    0.6147825121879578,\n",
      "    0.6140996813774109,\n",
      "    0.6071546673774719,\n",
      "    0.603735089302063,\n",
      "    0.6013337969779968,\n",
      "    0.6004250645637512\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "# C·∫•u h√¨nh Redis (ƒë·∫£m b·∫£o gi·ªëng CONFIG trong script g·ªëc)\n",
    "redis_client = redis.Redis(\n",
    "    host=\"localhost\",        # Ho·∫∑c thay b·∫±ng gi√° tr·ªã t·ª´ CONFIG[\"redis_host\"]\n",
    "    port=6379,               # CONFIG[\"redis_port\"]\n",
    "    db=0,                    # CONFIG[\"redis_db\"]\n",
    "    decode_responses=True,   # ƒê·∫£m b·∫£o tr·∫£ v·ªÅ string thay v√¨ bytes\n",
    "    password=\"123456\"    # N·∫øu c√≥ auth\n",
    ")\n",
    "\n",
    "# Item ID c·∫ßn truy v·∫•n\n",
    "item_id = \"B0002YV94U\"\n",
    "key = f\"rec:{item_id}\"\n",
    "\n",
    "# L·∫•y d·ªØ li·ªáu t·ª´ Redis\n",
    "rec_json = redis_client.get(key)\n",
    "\n",
    "if rec_json:\n",
    "    rec_data = json.loads(rec_json)\n",
    "    print(\"‚úÖ Recommendation found:\")\n",
    "    print(json.dumps(rec_data, indent=2))\n",
    "else:\n",
    "    print(f\"‚ùå No recommendation found for item {item_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db2bc6",
   "metadata": {},
   "source": [
    "## Load popular item to Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f932e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Top popular parent_asin (score-based):\n",
      "     parent_asin  rating_count  rating_avg  score\n",
      "3934  B0BW3QTWJJ           438    4.780822  780.0\n",
      "2559  B07C4NGT17           226    4.867257  422.0\n",
      "801   B0054TRQA4           216    4.750000  378.0\n",
      "1223  B00D8STBHY           202    4.816832  367.0\n",
      "3589  B09QPXVW35           166    4.885542  313.0\n",
      "...          ...           ...         ...    ...\n",
      "91    B000067NXE            45    4.333333   60.0\n",
      "3290  B08PMPDGXM            36    4.638889   59.0\n",
      "311   B000NV7L0I            42    4.404762   59.0\n",
      "103   B000088UPW            56    4.053571   59.0\n",
      "1451  B00IL7IFP6            33    4.787879   59.0\n",
      "\n",
      "[500 rows x 4 columns]\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u 500 popular parent_asin v√†o Redis key: popular_parent_asin_score\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import redis\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = os.path.abspath(\"../../.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "POSTGRES_URI = os.environ[\"POSTGRES_URI_OLTP\"]\n",
    "conn_str = POSTGRES_URI\n",
    "table = \"public.reviews\"\n",
    "\n",
    "# Redis config\n",
    "redis_client = redis.Redis(\n",
    "    host=\"localhost\",\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=True,\n",
    "    password=\"123456\"\n",
    ")\n",
    "\n",
    "# 1. Load t·ª´ PostgreSQL\n",
    "engine = create_engine(conn_str)\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        parent_asin,\n",
    "        rating\n",
    "    FROM {table}\n",
    "    WHERE parent_asin IS NOT NULL\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# 2. T√≠nh to√°n th·ªëng k√™\n",
    "agg_df = (\n",
    "    df.groupby(\"parent_asin\")\n",
    "    .agg(rating_count=(\"rating\", \"count\"), rating_avg=(\"rating\", \"mean\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 3. T√≠nh popularity score\n",
    "agg_df[\"score\"] = agg_df[\"rating_count\"] * (agg_df[\"rating_avg\"] - 3.0)\n",
    "\n",
    "# 4. L·∫•y top ph·ªï bi·∫øn theo score\n",
    "TOP_K = 500\n",
    "top_df = agg_df.sort_values(\"score\", ascending=False).head(TOP_K)\n",
    "\n",
    "# ‚úÖ In ra ƒë·ªÉ ki·ªÉm tra tr∆∞·ªõc khi l∆∞u\n",
    "print(\"üîç Top popular parent_asin (score-based):\")\n",
    "print(top_df[[\"parent_asin\", \"rating_count\", \"rating_avg\", \"score\"]])\n",
    "\n",
    "# 5. Ghi v√†o Redis n·∫øu OK\n",
    "redis_key = \"popular_parent_asin_score\"\n",
    "redis_client.delete(redis_key)\n",
    "\n",
    "for _, row in top_df.iterrows():\n",
    "    redis_client.zadd(redis_key, {row[\"parent_asin\"]: float(row[\"score\"])})\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ l∆∞u {len(top_df)} popular parent_asin v√†o Redis key: {redis_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b0a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Top 10 popular parent_asin from Redis:\n",
      " 1. ASIN: B0BW3QTWJJ | Score: 780.00\n",
      " 2. ASIN: B07C4NGT17 | Score: 422.00\n",
      " 3. ASIN: B0054TRQA4 | Score: 378.00\n",
      " 4. ASIN: B00D8STBHY | Score: 367.00\n",
      " 5. ASIN: B09QPXVW35 | Score: 313.00\n",
      " 6. ASIN: B07N29HQMN | Score: 257.00\n",
      " 7. ASIN: B00FZMDAO6 | Score: 248.00\n",
      " 8. ASIN: B0BG94QRLZ | Score: 241.00\n",
      " 9. ASIN: B09PH8LV57 | Score: 238.00\n",
      "10. ASIN: B004S8F7QM | Score: 238.00\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "\n",
    "# K·∫øt n·ªëi Redis\n",
    "redis_client = redis.Redis(\n",
    "    host=\"localhost\",\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=True,\n",
    "    password=\"123456\"\n",
    ")\n",
    "\n",
    "# Redis key b·∫°n ƒë√£ l∆∞u tr∆∞·ªõc ƒë√≥\n",
    "redis_key = \"popular_parent_asin_score\"\n",
    "\n",
    "# Truy v·∫•n top 10 ph·ªï bi·∫øn nh·∫•t (score cao nh·∫•t)\n",
    "top_items = redis_client.zrevrange(redis_key, 0, 9, withscores=True)\n",
    "\n",
    "# In ra k·∫øt qu·∫£\n",
    "print(\"üî• Top 10 popular parent_asin from Redis:\")\n",
    "for rank, (asin, score) in enumerate(top_items, start=1):\n",
    "    print(f\"{rank:2d}. ASIN: {asin} | Score: {score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datn-recsys)",
   "language": "python",
   "name": "datn-recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
